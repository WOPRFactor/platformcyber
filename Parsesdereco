# GU√çA DE IMPLEMENTACI√ìN - PARSERS DE RECONOCIMIENTO

**Versi√≥n**: 1.0  
**Fecha**: 10 Diciembre 2025  
**Fase**: Reconnaissance (21 parsers nuevos)

---

## üìã √çNDICE

1. [Contexto](#contexto)
2. [Estructura de Archivos](#estructura)
3. [Parsers de Subdominios (4 nuevos)](#subdominios)
4. [Parsers de DNS (4 nuevos)](#dns)
5. [Parsers de OSINT (5 nuevos)](#osint)
6. [Parsers Web (2 nuevos)](#web)
7. [Parsers Otros (3 nuevos)](#otros)
8. [Tests Unitarios](#tests)
9. [Registro en ParserManager](#registro)
10. [Validaci√≥n](#validacion)

---

## üìä CONTEXTO {#contexto}

### Estado Actual
- ‚úÖ **2 parsers** ya implementados: Subfinder, Amass
- ‚ùå **21 parsers** pendientes

### Objetivo
Implementar los 21 parsers restantes de la fase de reconocimiento, adaptados a los formatos reales de salida de cada herramienta.

### Formatos
- **TXT**: 12 herramientas (parseo l√≠nea por l√≠nea o regex)
- **JSON**: 9 herramientas (parseo estructurado)

---

## üìÅ ESTRUCTURA DE ARCHIVOS {#estructura}

```
services/reporting/parsers/reconnaissance/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ subdomain/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ subfinder_parser.py      # ‚úÖ Ya existe
‚îÇ   ‚îú‚îÄ‚îÄ amass_parser.py          # ‚úÖ Ya existe
‚îÇ   ‚îú‚îÄ‚îÄ assetfinder_parser.py    # ‚ùå NUEVO
‚îÇ   ‚îú‚îÄ‚îÄ sublist3r_parser.py      # ‚ùå NUEVO
‚îÇ   ‚îú‚îÄ‚îÄ findomain_parser.py      # ‚ùå NUEVO
‚îÇ   ‚îî‚îÄ‚îÄ crtsh_parser.py          # ‚ùå NUEVO
‚îú‚îÄ‚îÄ dns/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ dnsrecon_parser.py       # ‚ùå NUEVO
‚îÇ   ‚îú‚îÄ‚îÄ dnsenum_parser.py        # ‚ùå NUEVO
‚îÇ   ‚îú‚îÄ‚îÄ fierce_parser.py         # ‚ùå NUEVO
‚îÇ   ‚îî‚îÄ‚îÄ traceroute_parser.py     # ‚ùå NUEVO
‚îú‚îÄ‚îÄ osint/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ shodan_parser.py         # ‚ùå NUEVO
‚îÇ   ‚îú‚îÄ‚îÄ censys_parser.py         # ‚ùå NUEVO
‚îÇ   ‚îú‚îÄ‚îÄ theharvester_parser.py   # ‚ùå NUEVO
‚îÇ   ‚îú‚îÄ‚îÄ hunterio_parser.py       # ‚ùå NUEVO
‚îÇ   ‚îî‚îÄ‚îÄ wayback_parser.py        # ‚ùå NUEVO
‚îú‚îÄ‚îÄ web/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ whatweb_parser.py        # ‚ùå NUEVO
‚îÇ   ‚îî‚îÄ‚îÄ webcrawler_parser.py     # ‚ùå NUEVO
‚îî‚îÄ‚îÄ other/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ whois_parser.py          # ‚ùå NUEVO
    ‚îú‚îÄ‚îÄ googledorks_parser.py    # ‚ùå NUEVO
    ‚îî‚îÄ‚îÄ secrets_parser.py        # ‚ùå NUEVO
```

---

## üåê PARSERS DE SUBDOMINIOS {#subdominios}

### 1. AssetfinderParser

**Archivo**: `services/reporting/parsers/reconnaissance/subdomain/assetfinder_parser.py`

```python
"""
Parser para Assetfinder - Subdomain enumeration.
Formato: TXT con un subdominio por l√≠nea.
"""

from pathlib import Path
from typing import List
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class AssetfinderParser(BaseParser):
    """Parser para archivos de Assetfinder."""
    
    def can_parse(self, file_path: Path) -> bool:
        """
        Verifica si el archivo puede ser parseado.
        
        Criterios:
        - Nombre contiene 'assetfinder'
        - Extensi√≥n .txt
        """
        filename = file_path.name.lower()
        return 'assetfinder' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de Assetfinder.
        
        Formato esperado (TXT):
        subdomain1.example.com
        subdomain2.example.com
        api.example.com
        
        Returns:
            Lista de findings con subdominios encontrados
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                logger.warning(f"Empty file: {file_path}")
                return findings
            
            lines = content.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                
                # Saltar l√≠neas vac√≠as o comentarios
                if not line or line.startswith('#'):
                    continue
                
                # Validar que parece un dominio (contiene punto)
                if '.' not in line:
                    continue
                
                # Crear finding
                finding = ParsedFinding(
                    title=f"Subdomain discovered: {line}",
                    severity='info',
                    description=f"Subdomain enumeration found: {line}",
                    category='reconnaissance',
                    affected_target=line,
                    evidence=f"Source: Assetfinder",
                    remediation=None,
                    cvss_score=None,
                    cve_id=None,
                    references=None,
                    raw_data={
                        'tool': 'assetfinder',
                        'subdomain': line,
                        'type': 'subdomain_enumeration'
                    }
                )
                findings.append(finding)
            
            logger.info(f"Assetfinder: Parsed {len(findings)} subdomains from {file_path}")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing Assetfinder file {file_path}: {e}")
            return findings
```

---

### 2. Sublist3rParser

**Archivo**: `services/reporting/parsers/reconnaissance/subdomain/sublist3r_parser.py`

```python
"""
Parser para Sublist3r - Subdomain enumeration.
Formato: TXT con un subdominio por l√≠nea (id√©ntico a Assetfinder).
"""

from pathlib import Path
from typing import List
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class Sublist3rParser(BaseParser):
    """Parser para archivos de Sublist3r."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'sublist3r' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de Sublist3r.
        
        Formato id√©ntico a Assetfinder.
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                
                if not line or line.startswith('#') or '.' not in line:
                    continue
                
                finding = ParsedFinding(
                    title=f"Subdomain discovered: {line}",
                    severity='info',
                    description=f"Subdomain enumeration found: {line}",
                    category='reconnaissance',
                    affected_target=line,
                    evidence=f"Source: Sublist3r",
                    raw_data={
                        'tool': 'sublist3r',
                        'subdomain': line,
                        'type': 'subdomain_enumeration'
                    }
                )
                findings.append(finding)
            
            logger.info(f"Sublist3r: Parsed {len(findings)} subdomains")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing Sublist3r file {file_path}: {e}")
            return findings
```

---

### 3. FindomainParser

**Archivo**: `services/reporting/parsers/reconnaissance/subdomain/findomain_parser.py`

```python
"""
Parser para Findomain - Subdomain enumeration.
"""

from pathlib import Path
from typing import List
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class FindomainParser(BaseParser):
    """Parser para archivos de Findomain."""
    
    def can_parse(self, file_path: Path) -> bool:
        filename = file_path.name.lower()
        return 'findomain' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            for line in content.strip().split('\n'):
                line = line.strip()
                if not line or '.' not in line:
                    continue
                
                finding = ParsedFinding(
                    title=f"Subdomain discovered: {line}",
                    severity='info',
                    description=f"Subdomain found: {line}",
                    category='reconnaissance',
                    affected_target=line,
                    evidence=f"Source: Findomain",
                    raw_data={'tool': 'findomain', 'subdomain': line}
                )
                findings.append(finding)
            
            logger.info(f"Findomain: Parsed {len(findings)} subdomains")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing Findomain: {e}")
            return findings
```

---

### 4. CrtshParser

**Archivo**: `services/reporting/parsers/reconnaissance/subdomain/crtsh_parser.py`

```python
"""
Parser para crt.sh - Certificate Transparency logs.
"""

from pathlib import Path
from typing import List
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class CrtshParser(BaseParser):
    """Parser para archivos de crt.sh."""
    
    def can_parse(self, file_path: Path) -> bool:
        filename = file_path.name.lower()
        return 'crtsh' in filename or 'crt.sh' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            for line in content.strip().split('\n'):
                line = line.strip()
                if not line or '.' not in line:
                    continue
                
                finding = ParsedFinding(
                    title=f"Subdomain from certificate: {line}",
                    severity='info',
                    description=f"Subdomain found in CT logs: {line}",
                    category='reconnaissance',
                    affected_target=line,
                    evidence=f"Source: crt.sh (Certificate Transparency)",
                    raw_data={'tool': 'crtsh', 'subdomain': line}
                )
                findings.append(finding)
            
            logger.info(f"crt.sh: Parsed {len(findings)} subdomains")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing crt.sh: {e}")
            return findings
```

---

## üîç PARSERS DE DNS {#dns}

### 5. DNSReconParser

**Archivo**: `services/reporting/parsers/reconnaissance/dns/dnsrecon_parser.py`

```python
"""
Parser para DNSRecon - DNS enumeration.
Formato: JSON con registros DNS (SOA, NS, MX, A, TXT, CNAME).
"""

from pathlib import Path
from typing import List
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class DNSReconParser(BaseParser):
    """Parser para archivos JSON de DNSRecon."""
    
    def can_parse(self, file_path: Path) -> bool:
        filename = file_path.name.lower()
        return 'dnsrecon' in filename and file_path.suffix == '.json'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo JSON de DNSRecon.
        
        Formato esperado:
        [
            {
                "type": "SOA",
                "domain": "example.com",
                "address": "192.0.2.1",
                "mname": "ns1.example.com"
            },
            {
                "type": "A",
                "name": "example.com",
                "address": "192.0.2.1"
            }
        ]
        """
        findings = []
        
        try:
            data = self._safe_parse_json(file_path)
            if not data or not isinstance(data, list):
                logger.warning(f"Invalid DNSRecon JSON format: {file_path}")
                return findings
            
            for record in data:
                record_type = record.get('type', 'UNKNOWN')
                
                # Ignorar ScanInfo (metadata)
                if record_type == 'ScanInfo':
                    continue
                
                domain = record.get('domain') or record.get('name', 'unknown')
                address = record.get('address', '')
                
                # Construir descripci√≥n seg√∫n tipo de registro
                if record_type == 'SOA':
                    description = f"SOA record: {domain} ‚Üí {record.get('mname', 'N/A')}"
                elif record_type == 'NS':
                    description = f"Name server: {record.get('target', address)}"
                elif record_type == 'MX':
                    description = f"Mail server: {record.get('exchange', address)}"
                elif record_type == 'A' or record_type == 'AAAA':
                    description = f"DNS record: {domain} ‚Üí {address}"
                elif record_type == 'TXT':
                    description = f"TXT record: {record.get('strings', 'N/A')}"
                elif record_type == 'CNAME':
                    description = f"CNAME: {domain} ‚Üí {record.get('target', 'N/A')}"
                else:
                    description = f"{record_type} record for {domain}"
                
                finding = ParsedFinding(
                    title=f"DNS {record_type} record: {domain}",
                    severity='info',
                    description=description,
                    category='dns_enumeration',
                    affected_target=domain,
                    evidence=f"Address: {address}" if address else None,
                    raw_data={
                        'tool': 'dnsrecon',
                        'record_type': record_type,
                        'domain': domain,
                        'address': address,
                        'full_record': record
                    }
                )
                findings.append(finding)
            
            logger.info(f"DNSRecon: Parsed {len(findings)} DNS records")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing DNSRecon file {file_path}: {e}")
            return findings
```

---

### 6. FierceParser

**Archivo**: `services/reporting/parsers/reconnaissance/dns/fierce_parser.py`

```python
"""
Parser para Fierce - DNS reconnaissance.
Formato: TXT con estructura de NS, SOA, subdominios encontrados.
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class FierceParser(BaseParser):
    """Parser para archivos de Fierce."""
    
    def can_parse(self, file_path: Path) -> bool:
        filename = file_path.name.lower()
        return 'fierce' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de Fierce.
        
        Formato esperado:
        NS: ns1.example.com. ns2.example.com.
        SOA: ns1.example.com. (192.0.2.1)
        Zone: failure
        Wildcard: failure
        Found: ftp.example.com. (192.0.2.10)
        Nearby:
        {'192.0.2.9': 'server1.example.com.'}
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.split('\n')
            domain_base = None
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # Extraer Name Servers
                if line.startswith('NS:'):
                    ns_servers = line.replace('NS:', '').strip().split()
                    for ns in ns_servers:
                        ns = ns.rstrip('.')
                        finding = ParsedFinding(
                            title=f"Name Server: {ns}",
                            severity='info',
                            description=f"Name server discovered: {ns}",
                            category='dns_enumeration',
                            affected_target=ns,
                            evidence="Source: Fierce DNS scan",
                            raw_data={'tool': 'fierce', 'type': 'NS', 'server': ns}
                        )
                        findings.append(finding)
                
                # Extraer SOA
                elif line.startswith('SOA:'):
                    match = re.search(r'SOA:\s+([\w\.\-]+)\.?\s+\(([\d\.]+)\)', line)
                    if match:
                        soa_server = match.group(1)
                        soa_ip = match.group(2)
                        finding = ParsedFinding(
                            title=f"SOA Record: {soa_server}",
                            severity='info',
                            description=f"SOA server: {soa_server} ({soa_ip})",
                            category='dns_enumeration',
                            affected_target=soa_server,
                            evidence=f"IP: {soa_ip}",
                            raw_data={'tool': 'fierce', 'type': 'SOA', 'server': soa_server, 'ip': soa_ip}
                        )
                        findings.append(finding)
                
                # Extraer subdominios encontrados
                elif line.startswith('Found:'):
                    match = re.search(r'Found:\s+([\w\.\-]+)\.?\s+\(([\d\.]+)\)', line)
                    if match:
                        subdomain = match.group(1)
                        ip = match.group(2)
                        finding = ParsedFinding(
                            title=f"Subdomain discovered: {subdomain}",
                            severity='info',
                            description=f"Subdomain found via DNS bruteforce: {subdomain}",
                            category='reconnaissance',
                            affected_target=subdomain,
                            evidence=f"IP: {ip}",
                            raw_data={'tool': 'fierce', 'type': 'subdomain', 'domain': subdomain, 'ip': ip}
                        )
                        findings.append(finding)
                
                # Extraer IPs cercanas (nearby)
                elif "'" in line and ':' in line:
                    # Parsear diccionario Python: {'192.0.2.9': 'server1.example.com.'}
                    match = re.search(r"'([\d\.]+)':\s+'([\w\.\-]+)'", line)
                    if match:
                        ip = match.group(1)
                        hostname = match.group(2).rstrip('.')
                        finding = ParsedFinding(
                            title=f"Nearby host: {hostname}",
                            severity='info',
                            description=f"Nearby IP found: {ip} ‚Üí {hostname}",
                            category='reconnaissance',
                            affected_target=hostname,
                            evidence=f"IP: {ip}",
                            raw_data={'tool': 'fierce', 'type': 'nearby', 'ip': ip, 'hostname': hostname}
                        )
                        findings.append(finding)
            
            logger.info(f"Fierce: Parsed {len(findings)} DNS findings")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing Fierce file {file_path}: {e}")
            return findings
```

---

### 7. DNSEnumParser

**Archivo**: `services/reporting/parsers/reconnaissance/dns/dnsenum_parser.py`

```python
"""
Parser para DNSEnum - DNS enumeration.
Formato: TXT con secciones (Host's addresses, Name Servers, MX records).
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class DNSEnumParser(BaseParser):
    """Parser para archivos de DNSEnum."""
    
    def can_parse(self, file_path: Path) -> bool:
        filename = file_path.name.lower()
        return 'dnsenum' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de DNSEnum.
        
        Formato:
        Host's addresses:
        example.com.    5    IN    A    192.0.2.1
        
        Name Servers:
        ns1.example.com.    5    IN    A    192.0.2.10
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.split('\n')
            current_section = None
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # Detectar secciones
                if "Host's addresses:" in line:
                    current_section = 'A_RECORDS'
                    continue
                elif "Name Servers:" in line:
                    current_section = 'NS_RECORDS'
                    continue
                elif "Mail" in line and "Servers:" in line:
                    current_section = 'MX_RECORDS'
                    continue
                
                # Parsear registros DNS (formato: domain.com. 5 IN A 192.0.2.1)
                match = re.search(r'([\w\.\-]+)\.\s+\d+\s+IN\s+(A|AAAA|NS|MX)\s+([\d\.\w\-]+)', line)
                if match:
                    domain = match.group(1)
                    record_type = match.group(2)
                    value = match.group(3)
                    
                    if current_section == 'A_RECORDS':
                        description = f"Host address: {domain} ‚Üí {value}"
                    elif current_section == 'NS_RECORDS':
                        description = f"Name server: {domain} ‚Üí {value}"
                    elif current_section == 'MX_RECORDS':
                        description = f"Mail server: {domain} ‚Üí {value}"
                    else:
                        description = f"{record_type} record: {domain} ‚Üí {value}"
                    
                    finding = ParsedFinding(
                        title=f"DNS {record_type}: {domain}",
                        severity='info',
                        description=description,
                        category='dns_enumeration',
                        affected_target=domain,
                        evidence=f"Value: {value}",
                        raw_data={
                            'tool': 'dnsenum',
                            'type': record_type,
                            'domain': domain,
                            'value': value,
                            'section': current_section
                        }
                    )
                    findings.append(finding)
            
            logger.info(f"DNSEnum: Parsed {len(findings)} DNS records")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing DNSEnum file {file_path}: {e}")
            return findings
```

---

### 8. TracerouteParser

**Archivo**: `services/reporting/parsers/reconnaissance/dns/traceroute_parser.py`

```python
"""
Parser para Traceroute - Network path tracing.
Formato: TXT con saltos (hops), l√≠nea por salto.
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class TracerouteParser(BaseParser):
    """Parser para archivos de Traceroute."""
    
    def can_parse(self, file_path: Path) -> bool:
        filename = file_path.name.lower()
        return 'traceroute' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de Traceroute.
        
        Formato:
        traceroute to example.com (192.0.2.1), 30 hops max
         1  192.168.0.1 (192.168.0.1)  5.203 ms
         2  10.42.64.1 (10.42.64.1)  9.512 ms
         5  * * *
        16  vps-123.example.com (192.0.2.1)  151.231 ms
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.split('\n')
            target_domain = None
            target_ip = None
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # Primera l√≠nea: traceroute to example.com (192.0.2.1)
                if line.startswith('traceroute to'):
                    match = re.search(r'traceroute to ([\w\.\-]+) \(([\d\.]+)\)', line)
                    if match:
                        target_domain = match.group(1)
                        target_ip = match.group(2)
                    continue
                
                # Saltos: formato " 1  192.168.0.1 (192.168.0.1)  5.203 ms"
                match = re.search(r'^\s*(\d+)\s+([\w\.\-]+)\s+\(([\d\.]+)\)\s+([\d\.]+)\s*ms', line)
                if match:
                    hop_number = int(match.group(1))
                    hostname = match.group(2)
                    ip = match.group(3)
                    latency = float(match.group(4))
                    
                    finding = ParsedFinding(
                        title=f"Traceroute hop {hop_number}: {hostname}",
                        severity='info',
                        description=f"Network hop: {hostname} ({ip}) - {latency}ms",
                        category='reconnaissance',
                        affected_target=hostname,
                        evidence=f"Hop: {hop_number}, IP: {ip}, Latency: {latency}ms",
                        raw_data={
                            'tool': 'traceroute',
                            'hop': hop_number,
                            'hostname': hostname,
                            'ip': ip,
                            'latency_ms': latency,
                            'target_domain': target_domain,
                            'target_ip': target_ip
                        }
                    )
                    findings.append(finding)
                
                # Timeouts: " 5  * * *"
                elif re.search(r'^\s*(\d+)\s+\*\s+\*\s+\*', line):
                    hop_number = int(re.search(r'^\s*(\d+)', line).group(1))
                    finding = ParsedFinding(
                        title=f"Traceroute hop {hop_number}: Timeout",
                        severity='info',
                        description=f"Hop {hop_number} timed out (filtered or unreachable)",
                        category='reconnaissance',
                        affected_target=target_domain or 'unknown',
                        evidence="No response received",
                        raw_data={
                            'tool': 'traceroute',
                            'hop': hop_number,
                            'status': 'timeout'
                        }
                    )
                    findings.append(finding)
            
            logger.info(f"Traceroute: Parsed {len(findings)} hops")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing Traceroute file {file_path}: {e}")
            return findings
```

---

## üîé PARSERS DE OSINT {#osint}

### 9. ShodanParser

**Archivo**: `services/reporting/parsers/reconnaissance/osint/shodan_parser.py`

```python
"""
Parser para Shodan - Internet-wide scanning.
Formato: JSON con matches (ip_str, port, product, version, vulns).
"""

from pathlib import Path
from typing import List
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class ShodanParser(BaseParser):
    """Parser para archivos JSON de Shodan."""
    
    def can_parse(self, file_path: Path) -> bool:
        filename = file_path.name.lower()
        return 'shodan' in filename and file_path.suffix == '.json'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo JSON de Shodan.
        
        Formato:
        {
          "total": 12,
          "matches": [
            {
              "ip_str": "192.0.2.1",
              "port": 443,
              "product": "nginx",
              "version": "1.18.0",
              "os": "Linux",
              "hostnames": ["example.com"],
              "location": {"country_code": "US", "city": "San Francisco"},
              "ssl": {"cert": {"issued": "2023-01-15"}},
              "vulns": ["CVE-2021-44228"]
            }
          ]
        }
        """
        findings = []
        
        try:
            data = self._safe_parse_json(file_path)
            if not data:
                return findings
            
            matches = data.get('matches', [])
            
            for match in matches:
                ip = match.get('ip_str', 'unknown')
                port = match.get('port', 0)
                product = match.get('product', 'unknown')
                version = match.get('version', '')
                os = match.get('os', '')
                hostnames = match.get('hostnames', [])
                vulns = match.get('vulns', [])
                
                # Determinar severidad
                if vulns:
                    severity = 'high'  # Tiene vulnerabilidades conocidas
                    title = f"Shodan: {ip}:{port} - {product} with vulnerabilities"
                else:
                    severity = 'info'
                    title = f"Shodan: {ip}:{port} - {product}"
                
                # Construir descripci√≥n
                description_parts = [f"Service: {product}"]
                if version:
                    description_parts.append(f"Version: {version}")
                if os:
                    description_parts.append(f"OS: {os}")
                if hostnames:
                    description_parts.append(f"Hostnames: {', '.join(hostnames)}")
                
                description = " | ".join(description_parts)
                
                # Evidencia con vulnerabilidades
                evidence = None
                if vulns:
                    evidence = f"Known vulnerabilities: {', '.join(vulns)}"
                
                # Location info
                location = match.get('location', {})
                location_str = f"{location.get('city', '')}, {location.get('country_code', '')}"
                
                finding = ParsedFinding(
                    title=title,
                    severity=severity,
                    description=description,
                    category='osint',
                    affected_target=f"{ip}:{port}",
                    evidence=evidence,
                    remediation="Review exposed services and patch known vulnerabilities" if vulns else None,
                    cve_id=vulns[0] if vulns else None,
                    references=[f"https://www.shodan.io/host/{ip}"] if ip != 'unknown' else None,
                    raw_data={
                        'tool': 'shodan',
                        'ip': ip,
                        'port': port,
                        'product': product,
                        'version': version,
                        'os': os,
                        'hostnames': hostnames,
                        'vulnerabilities': vulns,
                        'location': location_str,
                        'full_match': match
                    }
                )
                findings.append(finding)
            
            logger.info(f"Shodan: Parsed {len(findings)} service discoveries")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing Shodan file {file_path}: {e}")
            return findings
```

---

### 10. CensysParser

**Archivo**: `services/reporting/parsers/reconnaissance/osint/censys_parser.py`

```python
"""
Parser para Censys - Internet-wide certificate and service scanning.
Formato: JSON con certificados SSL/TLS y servicios.
"""

from pathlib import Path
from typing import List
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class CensysParser(BaseParser):
    """Parser para archivos JSON de Censys."""
    
    def can_parse(self, file_path: Path) -> bool:
        filename = file_path.name.lower()
        return 'censys' in filename and file_path.suffix == '.json'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo JSON de Censys.
        
        Formato:
        {
          "status": "ok",
          "results": [
            {
              "ip": "192.0.2.1",
              "protocols": ["443/https", "80/http"],
              "services": [
                {
                  "port": 443,
                  "certificate": {
                    "parsed": {
                      "subject_dn": "CN=example.com"
                    }
                  }
                }
              ]
            }
          ]
        }
        """
        findings = []
        
        try:
            data = self._safe_parse_json(file_path)
            if not data:
                return findings
            
            results = data.get('results', [])
            
            for result in results:
                ip = result.get('ip', 'unknown')
                protocols = result.get('protocols', [])
                services = result.get('services', [])
                
                # Finding por cada servicio
                for service in services:
                    port = service.get('port', 0)
                    
                    # Extraer info de certificado si existe
                    cert_info = service.get('certificate', {}).get('parsed', {})
                    subject_dn = cert_info.get('subject_dn', '')
                    
                    if subject_dn:
                        title = f"Censys: {ip}:{port} - Certificate for {subject_dn}"
                        description = f"SSL/TLS certificate found: {subject_dn}"
                        evidence = f"Subject DN: {subject_dn}"
                    else:
                        title = f"Censys: {ip}:{port} - Service discovered"
                        description = f"Service running on {ip}:{port}"
                        evidence = None
                    
                    finding = ParsedFinding(
                        title=title,
                        severity='info',
                        description=description,
                        category='osint',
                        affected_target=f"{ip}:{port}",
                        evidence=evidence,
                        raw_data={
                            'tool': 'censys',
                            'ip': ip,
                            'port': port,
                            'certificate': cert_info,
                            'protocols': protocols
                        }
                    )
                    findings.append(finding)
            
            logger.info(f"Censys: Parsed {len(findings)} service discoveries")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing Censys file {file_path}: {e}")
            return findings
```

---

### 11. TheHarvesterParser

**Archivo**: `services/reporting/parsers/reconnaissance/osint/theharvester_parser.py`

```python
"""
Parser para theHarvester - OSINT gathering (emails, hosts, IPs).
Formato: JSON con hosts, emails, ips, urls.
"""

from pathlib import Path
from typing import List
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class TheHarvesterParser(BaseParser):
    """Parser para archivos JSON de theHarvester."""
    
    def can_parse(self, file_path: Path) -> bool:
        filename = file_path.name.lower()
        return 'theharvester' in filename or 'harvester' in filename and file_path.suffix == '.json'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo JSON de theHarvester.
        
        Formato:
        {
          "hosts": ["www.example.com", "api.example.com"],
          "emails": ["admin@example.com"],
          "ips": ["192.0.2.1"],
          "urls": ["https://www.example.com"],
          "interesting_urls": ["https://example.com/admin"]
        }
        """
        findings = []
        
        try:
            data = self._safe_parse_json(file_path)
            if not data:
                return findings
            
            # Parsear hosts
            hosts = data.get('hosts', [])
            for host in hosts:
                finding = ParsedFinding(
                    title=f"Host discovered: {host}",
                    severity='info',
                    description=f"Host found via OSINT: {host}",
                    category='reconnaissance',
                    affected_target=host,
                    evidence="Source: theHarvester OSINT",
                    raw_data={'tool': 'theharvester', 'type': 'host', 'value': host}
                )
                findings.append(finding)
            
            # Parsear emails
            emails = data.get('emails', [])
            for email in emails:
                finding = ParsedFinding(
                    title=f"Email discovered: {email}",
                    severity='info',
                    description=f"Email address found: {email}",
                    category='osint',
                    affected_target=email,
                    evidence="Source: theHarvester OSINT",
                    remediation="Verify email exposure and consider SPF/DMARC policies",
                    raw_data={'tool': 'theharvester', 'type': 'email', 'value': email}
                )
                findings.append(finding)
            
            # Parsear IPs
            ips = data.get('ips', [])
            for ip in ips:
                finding = ParsedFinding(
                    title=f"IP address: {ip}",
                    severity='info',
                    description=f"IP address associated with target: {ip}",
                    category='reconnaissance',
                    affected_target=ip,
                    evidence="Source: theHarvester OSINT",
                    raw_data={'tool': 'theharvester', 'type': 'ip', 'value': ip}
                )
                findings.append(finding)
            
            # Parsear interesting URLs
            interesting_urls = data.get('interesting_urls', [])
            for url in interesting_urls:
                finding = ParsedFinding(
                    title=f"Interesting URL: {url}",
                    severity='low',  # Potentially sensitive
                    description=f"Potentially sensitive URL found: {url}",
                    category='osint',
                    affected_target=url,
                    evidence="Source: theHarvester OSINT",
                    remediation="Review access controls for sensitive paths",
                    raw_data={'tool': 'theharvester', 'type': 'interesting_url', 'value': url}
                )
                findings.append(finding)
            
            logger.info(f"theHarvester: Parsed {len(findings)} OSINT findings")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing theHarvester file {file_path}: {e}")
            return findings
```

---

### 12. HunterioParser

**Archivo**: `services/reporting/parsers/reconnaissance/osint/hunterio_parser.py`

```python
"""
Parser para Hunter.io - Email finder and verifier.
Formato: JSON con emails, pattern, confidence.
"""

from pathlib import Path
from typing import List
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class HunterioParser(BaseParser):
    """Parser para archivos JSON de Hunter.io."""
    
    def can_parse(self, file_path: Path) -> bool:
        filename = file_path.name.lower()
        return 'hunter' in filename or 'hunterio' in filename and file_path.suffix == '.json'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo JSON de Hunter.io.
        
        Formato:
        {
          "data": {
            "domain": "example.com",
            "pattern": "{first}.{last}",
            "emails": [
              {
                "value": "john.doe@example.com",
                "confidence": 95,
                "position": "Software Engineer"
              }
            ]
          }
        }
        """
        findings = []
        
        try:
            data = self._safe_parse_json(file_path)
            if not data:
                return findings
            
            data_section = data.get('data', {})
            domain = data_section.get('domain', 'unknown')
            pattern = data_section.get('pattern', 'unknown')
            emails = data_section.get('emails', [])
            
            # Finding por el pattern de emails
            if pattern and pattern != 'unknown':
                finding = ParsedFinding(
                    title=f"Email pattern discovered: {pattern}",
                    severity='info',
                    description=f"Email naming pattern for {domain}: {pattern}",
                    category='osint',
                    affected_target=domain,
                    evidence=f"Pattern: {pattern}",
                    remediation="Consider security awareness training for employees",
                    raw_data={'tool': 'hunterio', 'type': 'pattern', 'pattern': pattern, 'domain': domain}
                )
                findings.append(finding)
            
            # Finding por cada email
            for email_data in emails:
                email = email_data.get('value', '')
                confidence = email_data.get('confidence', 0)
                position = email_data.get('position', '')
                
                # Determinar severidad por confidence
                if confidence >= 90:
                    severity = 'low'  # Alta confianza = m√°s riesgo
                else:
                    severity = 'info'
                
                description = f"Email found: {email}"
                if position:
                    description += f" (Position: {position})"
                description += f" - Confidence: {confidence}%"
                
                finding = ParsedFinding(
                    title=f"Email discovered: {email}",
                    severity=severity,
                    description=description,
                    category='osint',
                    affected_target=email,
                    evidence=f"Confidence: {confidence}%, Position: {position}",
                    remediation="Review email exposure and consider anti-phishing measures",
                    raw_data={
                        'tool': 'hunterio',
                        'type': 'email',
                        'email': email,
                        'confidence': confidence,
                        'position': position
                    }
                )
                findings.append(finding)
            
            logger.info(f"Hunter.io: Parsed {len(findings)} email findings")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing Hunter.io file {file_path}: {e}")
            return findings
```

---

### 13. WaybackParser

**Archivo**: `services/reporting/parsers/reconnaissance/osint/wayback_parser.py`

```python
"""
Parser para Wayback Machine - Historical URL snapshots.
Formato: TXT con URLs que incluyen timestamp.
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class WaybackParser(BaseParser):
    """Parser para archivos de Wayback Machine."""
    
    def can_parse(self, file_path: Path) -> bool:
        filename = file_path.name.lower()
        return 'wayback' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de Wayback Machine.
        
        Formato:
        https://web.archive.org/web/20200101/http://example.com/
        https://web.archive.org/web/20200615/http://example.com/admin
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                if not line or not line.startswith('http'):
                    continue
                
                # Extraer timestamp y URL original
                # Formato: https://web.archive.org/web/YYYYMMDD/ORIGINAL_URL
                match = re.search(r'web\.archive\.org/web/(\d+)/(https?://[^\s]+)', line)
                if match:
                    timestamp = match.group(1)
                    original_url = match.group(2)
                    
                    # Identificar URLs potencialmente sensibles
                    sensitive_keywords = ['admin', 'login', 'config', 'backup', 'password', 'api', 'key']
                    is_sensitive = any(keyword in original_url.lower() for keyword in sensitive_keywords)
                    
                    severity = 'low' if is_sensitive else 'info'
                    
                    finding = ParsedFinding(
                        title=f"Historical URL: {original_url}",
                        severity=severity,
                        description=f"URL found in Wayback Machine (snapshot: {timestamp})",
                        category='osint',
                        affected_target=original_url,
                        evidence=f"Archive URL: {line}",
                        remediation="Review historical exposure and verify current access controls" if is_sensitive else None,
                        raw_data={
                            'tool': 'wayback',
                            'timestamp': timestamp,
                            'original_url': original_url,
                            'archive_url': line,
                            'is_sensitive': is_sensitive
                        }
                    )
                    findings.append(finding)
            
            logger.info(f"Wayback Machine: Parsed {len(findings)} historical URLs")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing Wayback file {file_path}: {e}")
            return findings
```

---

## üåê PARSERS WEB {#web}

### 14. WhatWebParser

**Archivo**: `services/reporting/parsers/reconnaissance/web/whatweb_parser.py`

```python
"""
Parser para WhatWeb - Web technology fingerprinting.
Formato: JSON con plugins detectados (servidor, frameworks, CMS, etc).
"""

from pathlib import Path
from typing import List
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class WhatWebParser(BaseParser):
    """Parser para archivos JSON de WhatWeb."""
    
    def can_parse(self, file_path: Path) -> bool:
        filename = file_path.name.lower()
        return 'whatweb' in filename and file_path.suffix == '.json'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo JSON de WhatWeb.
        
        Formato:
        [
          {
            "target": "https://example.com",
            "http_status": 200,
            "plugins": {
              "HTTPServer": {"string": ["nginx/1.18.0"]},
              "X-Powered-By": {"string": ["PHP/7.4.3"]},
              "JQuery": {"version": ["3.5.1"]},
              "Bootstrap": {"version": ["4.5.0"]}
            }
          }
        ]
        """
        findings = []
        
        try:
            data = self._safe_parse_json(file_path)
            if not data or not isinstance(data, list):
                return findings
            
            for entry in data:
                target = entry.get('target', 'unknown')
                http_status = entry.get('http_status', 0)
                plugins = entry.get('plugins', {})
                
                technologies = []
                
                # Extraer tecnolog√≠as detectadas
                for plugin_name, plugin_data in plugins.items():
                    if isinstance(plugin_data, dict):
                        # Obtener versi√≥n o string
                        version_list = plugin_data.get('version', plugin_data.get('string', []))
                        if version_list:
                            tech_info = f"{plugin_name}: {version_list[0]}"
                            technologies.append(tech_info)
                
                if technologies:
                    # Determinar severidad basado en tecnolog√≠as conocidas con vulnerabilidades
                    severity = 'info'
                    vulnerable_techs = []
                    
                    # Check for outdated or vulnerable technologies
                    for tech in technologies:
                        tech_lower = tech.lower()
                        if any(old in tech_lower for old in ['php/5', 'php/7.0', 'php/7.1', 'jquery/1', 'jquery/2']):
                            severity = 'low'
                            vulnerable_techs.append(tech)
                    
                    description = f"Web technologies detected: {', '.join(technologies)}"
                    if vulnerable_techs:
                        description += f" (Potentially outdated: {', '.join(vulnerable_techs)})"
                    
                    finding = ParsedFinding(
                        title=f"Web fingerprinting: {target}",
                        severity=severity,
                        description=description,
                        category='web_reconnaissance',
                        affected_target=target,
                        evidence=f"Technologies: {', '.join(technologies)}",
                        remediation="Update outdated components to latest stable versions" if vulnerable_techs else None,
                        raw_data={
                            'tool': 'whatweb',
                            'target': target,
                            'http_status': http_status,
                            'technologies': technologies,
                            'vulnerable_technologies': vulnerable_techs,
                            'plugins': plugins
                        }
                    )
                    findings.append(finding)
            
            logger.info(f"WhatWeb: Parsed {len(findings)} technology fingerprints")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing WhatWeb file {file_path}: {e}")
            return findings
```

---

### 15. WebCrawlerParser

**Archivo**: `services/reporting/parsers/reconnaissance/web/webcrawler_parser.py`

```python
"""
Parser para Web Crawlers (Gospider, Hakrawler) - URL discovery.
Formato: TXT con lista de URLs encontradas.
"""

from pathlib import Path
from typing import List
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class WebCrawlerParser(BaseParser):
    """Parser para archivos de web crawlers (Gospider, Hakrawler)."""
    
    def can_parse(self, file_path: Path) -> bool:
        filename = file_path.name.lower()
        return (('gospider' in filename or 'hakrawler' in filename or 'crawler' in filename or 'crawl' in filename) 
                and file_path.suffix == '.txt')
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de web crawler.
        
        Formato:
        https://example.com/
        https://example.com/api/v1/users
        https://example.com/admin/login
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.strip().split('\n')
            sensitive_paths = []
            api_endpoints = []
            static_resources = []
            
            for line in lines:
                line = line.strip()
                if not line or not line.startswith('http'):
                    continue
                
                # Clasificar URLs
                line_lower = line.lower()
                
                # URLs sensibles (admin, config, backup, etc)
                sensitive_keywords = ['admin', 'login', 'config', 'backup', 'password', 'credential', 'secret', '.env', 'db_']
                if any(keyword in line_lower for keyword in sensitive_keywords):
                    sensitive_paths.append(line)
                    severity = 'medium'
                    category = 'web_vulnerability'
                    description = f"Potentially sensitive path found: {line}"
                
                # API endpoints
                elif '/api/' in line_lower or '/v1/' in line_lower or '/v2/' in line_lower:
                    api_endpoints.append(line)
                    severity = 'info'
                    category = 'web_reconnaissance'
                    description = f"API endpoint discovered: {line}"
                
                # Recursos est√°ticos (bajo inter√©s)
                elif any(ext in line_lower for ext in ['.jpg', '.png', '.gif', '.css', '.js', '.svg', '.woff']):
                    static_resources.append(line)
                    continue  # No crear finding para recursos est√°ticos
                
                else:
                    severity = 'info'
                    category = 'web_reconnaissance'
                    description = f"URL discovered: {line}"
                
                finding = ParsedFinding(
                    title=f"Crawled URL: {line}",
                    severity=severity,
                    description=description,
                    category=category,
                    affected_target=line,
                    evidence="Source: Web crawler",
                    remediation="Review access controls for sensitive paths" if severity == 'medium' else None,
                    raw_data={
                        'tool': 'webcrawler',
                        'url': line,
                        'is_sensitive': severity == 'medium',
                        'is_api': '/api/' in line_lower
                    }
                )
                findings.append(finding)
            
            logger.info(f"WebCrawler: Parsed {len(findings)} URLs ({len(sensitive_paths)} sensitive, {len(api_endpoints)} API)")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing WebCrawler file {file_path}: {e}")
            return findings
```

---

## üîê PARSERS OTROS {#otros}

### 16. WhoisParser

**Archivo**: `services/reporting/parsers/reconnaissance/other/whois_parser.py`

```python
"""
Parser para WHOIS - Domain registration information.
Formato: TXT con campos clave-valor.
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class WhoisParser(BaseParser):
    """Parser para archivos de WHOIS."""
    
    def can_parse(self, file_path: Path) -> bool:
        filename = file_path.name.lower()
        return 'whois' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo WHOIS.
        
        Formato:
        Domain Name: EXAMPLE.TECH
        Creation Date: 2020-06-24T10:15:05.0Z
        Registry Expiry Date: 2026-06-24T23:59:59.0Z
        Registrar: Example Registrar
        Name Server: NS1.EXAMPLE.COM
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.split('\n')
            
            whois_data = {}
            domain_name = None
            
            for line in lines:
                line = line.strip()
                if not line or line.startswith('%') or line.startswith('#'):
                    continue
                
                # Parsear campos clave: valor
                if ':' in line:
                    parts = line.split(':', 1)
                    if len(parts) == 2:
                        key = parts[0].strip()
                        value = parts[1].strip()
                        whois_data[key] = value
                        
                        # Capturar domain name
                        if 'Domain Name' in key and not domain_name:
                            domain_name = value.lower()
            
            if not domain_name:
                domain_name = 'unknown'
            
            # Extraer informaci√≥n relevante
            creation_date = whois_data.get('Creation Date', whois_data.get('Created Date', 'N/A'))
            expiry_date = whois_data.get('Registry Expiry Date', whois_data.get('Expiry Date', 'N/A'))
            registrar = whois_data.get('Registrar', 'N/A')
            name_servers = [v for k, v in whois_data.items() if 'Name Server' in k]
            
            # Crear finding principal con info del dominio
            description = f"Domain: {domain_name}"
            if creation_date != 'N/A':
                description += f" | Created: {creation_date}"
            if expiry_date != 'N/A':
                description += f" | Expires: {expiry_date}"
            if registrar != 'N/A':
                description += f" | Registrar: {registrar}"
            
            evidence_parts = []
            if name_servers:
                evidence_parts.append(f"Name Servers: {', '.join(name_servers)}")
            
            finding = ParsedFinding(
                title=f"WHOIS information: {domain_name}",
                severity='info',
                description=description,
                category='reconnaissance',
                affected_target=domain_name,
                evidence=' | '.join(evidence_parts) if evidence_parts else None,
                raw_data={
                    'tool': 'whois',
                    'domain': domain_name,
                    'creation_date': creation_date,
                    'expiry_date': expiry_date,
                    'registrar': registrar,
                    'name_servers': name_servers,
                    'full_data': whois_data
                }
            )
            findings.append(finding)
            
            logger.info(f"WHOIS: Parsed information for {domain_name}")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing WHOIS file {file_path}: {e}")
            return findings
```

---

### 17. GoogleDorksParser

**Archivo**: `services/reporting/parsers/reconnaissance/other/googledorks_parser.py`

```python
"""
Parser para Google Dorks - Sensitive information disclosure.
Formato: TXT con URLs sensibles encontradas.
"""

from pathlib import Path
from typing import List
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class GoogleDorksParser(BaseParser):
    """Parser para archivos de Google Dorks."""
    
    def can_parse(self, file_path: Path) -> bool:
        filename = file_path.name.lower()
        return ('googledork' in filename or 'dork' in filename or 'google' in filename) and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de Google Dorks.
        
        Formato:
        https://example.com/admin/config.php
        https://example.com/backup/db_backup.sql
        https://example.com/.env
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                if not line or not line.startswith('http'):
                    continue
                
                # Clasificar severidad basado en tipo de archivo/path
                line_lower = line.lower()
                
                # Cr√≠tico: backups, credenciales, keys
                if any(kw in line_lower for kw in ['.sql', 'backup', '.env', 'credentials', 'password', 'secret', 'key', 'token']):
                    severity = 'critical'
                    remediation = "IMMEDIATE: Remove exposed file and audit for data breach"
                
                # Alto: configuraciones, admin panels
                elif any(kw in line_lower for kw in ['config', 'admin', 'phpinfo', 'web.config', 'htaccess']):
                    severity = 'high'
                    remediation = "Remove or restrict access to configuration files"
                
                # Medio: logs, traces
                elif any(kw in line_lower for kw in ['.log', 'error', 'debug', 'trace']):
                    severity = 'medium'
                    remediation = "Remove or restrict access to log files"
                
                else:
                    severity = 'low'
                    remediation = "Review and remove if sensitive"
                
                finding = ParsedFinding(
                    title=f"Sensitive exposure: {line}",
                    severity=severity,
                    description=f"Potentially sensitive file/path exposed via search engines: {line}",
                    category='information_disclosure',
                    affected_target=line,
                    evidence="Source: Google Dorks",
                    remediation=remediation,
                    raw_data={
                        'tool': 'googledorks',
                        'url': line,
                        'exposure_type': self._classify_exposure(line_lower)
                    }
                )
                findings.append(finding)
            
            logger.info(f"GoogleDorks: Parsed {len(findings)} sensitive exposures")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing GoogleDorks file {file_path}: {e}")
            return findings
    
    def _classify_exposure(self, url: str) -> str:
        """Clasifica el tipo de exposici√≥n."""
        if '.sql' in url or 'backup' in url:
            return 'database_backup'
        elif '.env' in url or 'config' in url:
            return 'configuration'
        elif 'admin' in url or 'login' in url:
            return 'admin_panel'
        elif '.log' in url:
            return 'log_file'
        else:
            return 'other'
```

---

### 18. SecretsParser

**Archivo**: `services/reporting/parsers/reconnaissance/other/secrets_parser.py`

```python
"""
Parser para Secrets Detection (Gitleaks, Trufflehog) - Exposed secrets.
Formato: JSON con secrets encontrados (API keys, passwords, tokens).
"""

from pathlib import Path
from typing import List
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class SecretsParser(BaseParser):
    """Parser para archivos de herramientas de detecci√≥n de secretos."""
    
    def can_parse(self, file_path: Path) -> bool:
        filename = file_path.name.lower()
        return (('gitleaks' in filename or 'trufflehog' in filename or 'secret' in filename) 
                and file_path.suffix == '.json')
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo JSON de Gitleaks/Trufflehog.
        
        Formato:
        [
          {
            "Description": "AWS Access Key",
            "Match": "AKIAIOSFODNN7EXAMPLE",
            "File": "config/aws.js",
            "StartLine": 15,
            "RuleID": "aws-access-token",
            "Entropy": 4.5
          }
        ]
        """
        findings = []
        
        try:
            data = self._safe_parse_json(file_path)
            if not data or not isinstance(data, list):
                return findings
            
            for secret in data:
                description = secret.get('Description', 'Secret detected')
                match_value = secret.get('Match', '')
                file_path_str = secret.get('File', 'unknown')
                start_line = secret.get('StartLine', 0)
                rule_id = secret.get('RuleID', 'unknown')
                entropy = secret.get('Entropy', 0)
                
                # Determinar severidad basado en tipo de secreto
                description_lower = description.lower()
                if any(kw in description_lower for kw in ['password', 'api key', 'access key', 'secret key', 'private key']):
                    severity = 'critical'
                elif any(kw in description_lower for kw in ['token', 'credential']):
                    severity = 'high'
                else:
                    severity = 'medium'
                
                # Ocultar parte del secret (security)
                if match_value and len(match_value) > 10:
                    masked_value = match_value[:4] + '...' + match_value[-4:]
                else:
                    masked_value = '***'
                
                finding = ParsedFinding(
                    title=f"Exposed secret: {description}",
                    severity=severity,
                    description=f"Secret detected in {file_path_str} (line {start_line}): {description}",
                    category='secrets_exposure',
                    affected_target=file_path_str,
                    evidence=f"Rule: {rule_id}, Entropy: {entropy}, Value: {masked_value}",
                    remediation="IMMEDIATE: Rotate exposed credentials and remove from code/repository",
                    raw_data={
                        'tool': 'secrets_detection',
                        'description': description,
                        'file': file_path_str,
                        'line': start_line,
                        'rule_id': rule_id,
                        'entropy': entropy,
                        'match_length': len(match_value)
                    }
                )
                findings.append(finding)
            
            logger.info(f"SecretsDetection: Parsed {len(findings)} exposed secrets")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing Secrets file {file_path}: {e}")
            return findings
```

---

## üß™ TESTS UNITARIOS {#tests}

### Archivo de Tests

**Ubicaci√≥n**: `tests/unit/test_reconnaissance_parsers.py`

```python
"""
Tests unitarios para parsers de reconocimiento.
"""

import pytest
from pathlib import Path
from services.reporting.parsers.reconnaissance.subdomain import (
    assetfinder_parser, sublist3r_parser, findomain_parser, crtsh_parser
)
from services.reporting.parsers.reconnaissance.dns import (
    dnsrecon_parser, fierce_parser, dnsenum_parser, traceroute_parser
)
from services.reporting.parsers.reconnaissance.osint import (
    shodan_parser, censys_parser, theharvester_parser, hunterio_parser, wayback_parser
)
from services.reporting.parsers.reconnaissance.web import (
    whatweb_parser, webcrawler_parser
)
from services.reporting.parsers.reconnaissance.other import (
    whois_parser, googledorks_parser, secrets_parser
)

# Fixtures directory
FIXTURES_DIR = Path(__file__).parent.parent / 'fixtures' / 'reconnaissance'


class TestSubdomainParsers:
    """Tests para parsers de enumeraci√≥n de subdominios."""
    
    def test_assetfinder_parser(self):
        """Test AssetfinderParser."""
        parser = assetfinder_parser.AssetfinderParser()
        fixture_file = FIXTURES_DIR / 'assetfinder_sample.txt'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) > 0
        assert all(f.category == 'reconnaissance' for f in findings)
        assert all(f.severity == 'info' for f in findings)
        assert all('subdomain' in f.raw_data for f in findings)
    
    def test_sublist3r_parser(self):
        """Test Sublist3rParser."""
        parser = sublist3r_parser.Sublist3rParser()
        fixture_file = FIXTURES_DIR / 'sublist3r_sample.txt'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) > 0
        assert all('.' in f.affected_target for f in findings)


class TestDNSParsers:
    """Tests para parsers de DNS."""
    
    def test_dnsrecon_parser(self):
        """Test DNSReconParser."""
        parser = dnsrecon_parser.DNSReconParser()
        fixture_file = FIXTURES_DIR / 'dnsrecon_sample.json'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) > 0
        assert all(f.category == 'dns_enumeration' for f in findings)
        assert any(f.raw_data.get('record_type') == 'A' for f in findings)
    
    def test_fierce_parser(self):
        """Test FierceParser."""
        parser = fierce_parser.FierceParser()
        fixture_file = FIXTURES_DIR / 'fierce_sample.txt'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) > 0
        # Debe encontrar NS, SOA o subdominios
        assert any('NS' in f.raw_data.get('type', '') or 
                   'SOA' in f.raw_data.get('type', '') or 
                   'subdomain' in f.raw_data.get('type', '') 
                   for f in findings)


class TestOSINTParsers:
    """Tests para parsers de OSINT."""
    
    def test_shodan_parser(self):
        """Test ShodanParser."""
        parser = shodan_parser.ShodanParser()
        fixture_file = FIXTURES_DIR / 'shodan_sample.json'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) > 0
        assert all(f.category == 'osint' for f in findings)
        assert any(':' in f.affected_target for f in findings)  # IP:port format
    
    def test_theharvester_parser(self):
        """Test TheHarvesterParser."""
        parser = theharvester_parser.TheHarvesterParser()
        fixture_file = FIXTURES_DIR / 'theharvester_sample.json'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) > 0
        # Debe tener hosts o emails
        assert any(f.raw_data.get('type') in ['host', 'email', 'ip'] for f in findings)


class TestWebParsers:
    """Tests para parsers web."""
    
    def test_whatweb_parser(self):
        """Test WhatWebParser."""
        parser = whatweb_parser.WhatWebParser()
        fixture_file = FIXTURES_DIR / 'whatweb_sample.json'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) > 0
        assert all(f.category == 'web_reconnaissance' for f in findings)
        assert any('technologies' in f.raw_data for f in findings)
    
    def test_webcrawler_parser(self):
        """Test WebCrawlerParser."""
        parser = webcrawler_parser.WebCrawlerParser()
        fixture_file = FIXTURES_DIR / 'gospider_sample.txt'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) >= 0  # Puede no tener URLs sensibles
        if findings:
            assert all(f.affected_target.startswith('http') for f in findings)


class TestOtherParsers:
    """Tests para otros parsers."""
    
    def test_whois_parser(self):
        """Test WhoisParser."""
        parser = whois_parser.WhoisParser()
        fixture_file = FIXTURES_DIR / 'whois_sample.txt'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) > 0
        assert findings[0].category == 'reconnaissance'
        assert 'domain' in findings[0].raw_data
    
    def test_secrets_parser(self):
        """Test SecretsParser."""
        parser = secrets_parser.SecretsParser()
        fixture_file = FIXTURES_DIR / 'gitleaks_sample.json'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) > 0
        assert all(f.category == 'secrets_exposure' for f in findings)
        assert all(f.severity in ['critical', 'high', 'medium'] for f in findings)
```

---

### Crear Fixtures de Prueba

**Crear directorio**: `tests/fixtures/reconnaissance/`

**Archivos de ejemplo** (crear con contenido m√≠nimo):

```bash
# En tests/fixtures/reconnaissance/

# assetfinder_sample.txt
api.example.com
www.example.com
mail.example.com

# dnsrecon_sample.json
[
  {"type": "A", "name": "example.com", "address": "192.0.2.1"},
  {"type": "NS", "domain": "example.com", "target": "ns1.example.com"}
]

# shodan_sample.json
{
  "matches": [
    {
      "ip_str": "192.0.2.1",
      "port": 443,
      "product": "nginx",
      "hostnames": ["example.com"]
    }
  ]
}

# theharvester_sample.json
{
  "hosts": ["www.example.com"],
  "emails": ["admin@example.com"],
  "ips": ["192.0.2.1"]
}

# whatweb_sample.json
[
  {
    "target": "https://example.com",
    "plugins": {
      "HTTPServer": {"string": ["nginx/1.18.0"]},
      "JQuery": {"version": ["3.5.1"]}
    }
  }
]

# gitleaks_sample.json
[
  {
    "Description": "AWS Access Key",
    "Match": "AKIAIOSFODNN7EXAMPLE",
    "File": "config.js",
    "StartLine": 10,
    "RuleID": "aws-key"
  }
]
```

---

## üîó REGISTRO EN PARSERMANAGER {#registro}

### Actualizar ParserManager

**Archivo**: `services/reporting/parsers/parser_manager.py`

```python
# Agregar imports de los nuevos parsers

# Subdomain parsers
from .reconnaissance.subdomain.assetfinder_parser import AssetfinderParser
from .reconnaissance.subdomain.sublist3r_parser import Sublist3rParser
from .reconnaissance.subdomain.findomain_parser import FindomainParser
from .reconnaissance.subdomain.crtsh_parser import CrtshParser

# DNS parsers
from .reconnaissance.dns.dnsrecon_parser import DNSReconParser
from .reconnaissance.dns.fierce_parser import FierceParser
from .reconnaissance.dns.dnsenum_parser import DNSEnumParser
from .reconnaissance.dns.traceroute_parser import TracerouteParser

# OSINT parsers
from .reconnaissance.osint.shodan_parser import ShodanParser
from .reconnaissance.osint.censys_parser import CensysParser
from .reconnaissance.osint.theharvester_parser import TheHarvesterParser
from .reconnaissance.osint.hunterio_parser import HunterioParser
from .reconnaissance.osint.wayback_parser import WaybackParser

# Web parsers
from .reconnaissance.web.whatweb_parser import WhatWebParser
from .reconnaissance.web.webcrawler_parser import WebCrawlerParser

# Other parsers
from .reconnaissance.other.whois_parser import WhoisParser
from .reconnaissance.other.googledorks_parser import GoogleDorksParser
from .reconnaissance.other.secrets_parser import SecretsParser


class ParserManager:
    """Gestiona el registro y selecci√≥n de parsers."""
    
    def __init__(self):
        self.parsers = []
        self._register_parsers()
    
    def _register_parsers(self):
        """Registra todos los parsers disponibles."""
        
        # Parsers existentes
        self.parsers.extend([
            SubfinderParser(),
            AmassParser(),
            NmapParser(),
            NucleiParser(),
            NiktoParser(),
        ])
        
        # NUEVO: Parsers de reconocimiento
        self.parsers.extend([
            # Subdomain
            AssetfinderParser(),
            Sublist3rParser(),
            FindomainParser(),
            CrtshParser(),
            
            # DNS
            DNSReconParser(),
            FierceParser(),
            DNSEnumParser(),
            TracerouteParser(),
            
            # OSINT
            ShodanParser(),
            CensysParser(),
            TheHarvesterParser(),
            HunterioParser(),
            WaybackParser(),
            
            # Web
            WhatWebParser(),
            WebCrawlerParser(),
            
            # Other
            WhoisParser(),
            GoogleDorksParser(),
            SecretsParser(),
        ])
```

---

## ‚úÖ VALIDACI√ìN {#validacion}

### Checklist de Implementaci√≥n

- [ ] **Estructura creada**: Todos los directorios y `__init__.py`
- [ ] **21 parsers implementados**: C√≥digo completo con manejo de errores
- [ ] **ParserManager actualizado**: Parsers registrados
- [ ] **Tests creados**: `test_reconnaissance_parsers.py`
- [ ] **Fixtures creadas**: Archivos de ejemplo en `tests/fixtures/reconnaissance/`
- [ ] **Tests passing**: `pytest tests/unit/test_reconnaissance_parsers.py -v`

### Validaci√≥n Manual

```bash
# 1. Ejecutar tests
cd platform/backend
pytest tests/unit/test_reconnaissance_parsers.py -v

# 2. Test con workspace real
# Generar reporte y verificar que parsea archivos nuevos

# 3. Verificar logs
tail -f logs/flask.log | grep "Parser"
# Debe mostrar: "Assetfinder: Parsed X subdomains", etc.
```

### M√©tricas de √âxito

- ‚úÖ Tests unitarios passing (21 parsers)
- ‚úÖ Reportes incluyen findings de herramientas nuevas
- ‚úÖ Logs muestran parseo exitoso
- ‚úÖ No errores en archivos v√°lidos
- ‚úÖ Manejo graceful de archivos inv√°lidos

---

## üìù PROMPT PARA CURSOR

**Instrucci√≥n final**:

```
Lee completo este archivo (GUIA_PARSERS_RECONNAISSANCE.md) y luego:

1. Crea la estructura de directorios especificada en la secci√≥n "Estructura de Archivos"

2. Implementa los 21 parsers nuevos copiando exactamente el c√≥digo de las secciones:
   - Subdominios (4 parsers)
   - DNS (4 parsers)
   - OSINT (5 parsers)
   - Web (2 parsers)
   - Otros (3 parsers)

3. Actualiza ParserManager como se especifica en la secci√≥n "Registro"

4. Crea los tests unitarios y fixtures

5. Ejecuta los tests: pytest tests/unit/test_reconnaissance_parsers.py -v

Despu√©s de implementar cada subdirectorio (subdomain/, dns/, osint/, web/, other/),
av√≠same para validar antes de continuar con el siguiente.

Empieza por el directorio subdomain/ (4 parsers).
```

---

## üéØ RESUMEN

**Parsers implementados**: 21 nuevos  
**Tiempo estimado**: 5-6 horas  
**Complejidad**: Media  
**Beneficio**: Reportes 10x m√°s completos

**Resultado**: Sistema de reporter√≠a con cobertura completa de fase de reconocimiento (23 herramientas totales).

---

**Versi√≥n**: 1.0  
**Autor**: Claude AI  
**Fecha**: 10 Diciembre 2025