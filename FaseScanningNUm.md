# GU√çA DE IMPLEMENTACI√ìN - PARSERS DE SCANNING Y ENUMERATION

**Versi√≥n**: 1.0  
**Fecha**: 10 Diciembre 2025  
**Fase**: Scanning & Enumeration (32 parsers nuevos)

---

## üìã √çNDICE

1. [Contexto](#contexto)
2. [Estructura de Archivos](#estructura)
3. [FASE 1: Port Scanning (3 parsers)](#fase1)
4. [FASE 2: SMB & SSL Enumeration (5 parsers)](#fase2)
5. [FASE 3: Network Services (6 parsers)](#fase3)
6. [FASE 4: Database Enumeration (3 parsers)](#fase4)
7. [FASE 5: Vulnerability Assessment (4 parsers)](#fase5)
8. [Tests Unitarios](#tests)
9. [Registro en ParserManager](#registro)
10. [Validaci√≥n](#validacion)

---

## üìä CONTEXTO {#contexto}

### Estado Actual
- ‚úÖ **3 parsers** ya implementados: Nmap, Nuclei, Nikto
- ‚ùå **32 parsers** pendientes

### Objetivo
Implementar los 32 parsers restantes de scanning y enumeration, adaptados a los formatos reales de salida de cada herramienta.

### Formatos
- **TXT**: 23 herramientas (parseo estructurado, regex, secciones)
- **JSON**: 5 herramientas (parseo estructurado)
- **CSV**: 1 herramienta (SQLMap en directorio)
- **XML**: Ya cubierto por NmapParser existente

---

## üìÅ ESTRUCTURA DE ARCHIVOS {#estructura}

```
services/reporting/parsers/
‚îú‚îÄ‚îÄ scanning/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ nmap_parser.py          # ‚úÖ Ya existe
‚îÇ   ‚îú‚îÄ‚îÄ rustscan_parser.py      # ‚ùå NUEVO
‚îÇ   ‚îú‚îÄ‚îÄ masscan_parser.py       # ‚ùå NUEVO
‚îÇ   ‚îî‚îÄ‚îÄ naabu_parser.py         # ‚ùå NUEVO
‚îú‚îÄ‚îÄ enumeration/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ smb/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ enum4linux_parser.py    # ‚ùå NUEVO
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ smbmap_parser.py        # ‚ùå NUEVO
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ smbclient_parser.py     # ‚ùå NUEVO
‚îÇ   ‚îú‚îÄ‚îÄ ssl/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sslscan_parser.py       # ‚ùå NUEVO
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sslyze_parser.py        # ‚ùå NUEVO
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ testssl_parser.py       # ‚ùå NUEVO
‚îÇ   ‚îú‚îÄ‚îÄ network/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ssh_audit_parser.py     # ‚ùå NUEVO
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ smtp_enum_parser.py     # ‚ùå NUEVO
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dns_zone_parser.py      # ‚ùå NUEVO
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ snmpwalk_parser.py      # ‚ùå NUEVO
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ onesixtyone_parser.py   # ‚ùå NUEVO
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ldapsearch_parser.py    # ‚ùå NUEVO
‚îÇ   ‚îî‚îÄ‚îÄ database/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ mysql_enum_parser.py    # ‚ùå NUEVO
‚îÇ       ‚îú‚îÄ‚îÄ postgresql_enum_parser.py # ‚ùå NUEVO
‚îÇ       ‚îî‚îÄ‚îÄ redis_enum_parser.py    # ‚ùå NUEVO
‚îî‚îÄ‚îÄ vulnerability/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ nuclei_parser.py        # ‚úÖ Ya existe
    ‚îú‚îÄ‚îÄ nikto_parser.py         # ‚úÖ Ya existe
    ‚îú‚îÄ‚îÄ sqlmap_parser.py        # ‚ùå NUEVO
    ‚îú‚îÄ‚îÄ owasp_zap_parser.py     # ‚ùå NUEVO
    ‚îî‚îÄ‚îÄ wpscan_parser.py        # ‚ùå NUEVO
```

---

## üî¥ FASE 1: PORT SCANNING PARSERS {#fase1}

### 1. RustScanParser

**Archivo**: `services/reporting/parsers/scanning/rustscan_parser.py`

```python
"""
Parser para RustScan - Fast port scanner.
Formato: TXT con output similar a Nmap.
"""

from pathlib import Path
from typing import List
import re
import logging
from ..base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class RustScanParser(BaseParser):
    """Parser para archivos de RustScan."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'rustscan' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de RustScan.
        
        Formato esperado (TXT):
        Nmap scan report for 192.168.1.1
        Host is up (0.05s latency).
        
        PORT   STATE SERVICE VERSION
        22/tcp open  ssh     OpenSSH 8.2p1
        80/tcp open  http    Apache httpd 2.4.41
        443/tcp open  ssl/http Apache httpd 2.4.41
        
        O formato grepeable:
        Host: 192.168.1.1 (up) Ports: 22/open/tcp//ssh///, 80/open/tcp//http///
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.split('\n')
            current_host = None
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # Extraer host target
                if 'Nmap scan report for' in line or 'scan report for' in line.lower():
                    match = re.search(r'for\s+([\d\.]+)', line)
                    if match:
                        current_host = match.group(1)
                    continue
                
                # Parsear formato tabla: PORT   STATE SERVICE VERSION
                port_match = re.match(r'^(\d+)/(tcp|udp)\s+(open|closed|filtered)\s+(\S+)(?:\s+(.+))?', line)
                if port_match:
                    port = int(port_match.group(1))
                    protocol = port_match.group(2)
                    state = port_match.group(3)
                    service = port_match.group(4)
                    version = port_match.group(5) if port_match.group(5) else ''
                    
                    if state == 'open':
                        severity = 'info'
                        title = f"Open port: {port}/{protocol} ({service})"
                        description = f"Port {port}/{protocol} is open running {service}"
                        if version:
                            description += f" {version}"
                        
                        finding = ParsedFinding(
                            title=title,
                            severity=severity,
                            description=description,
                            category='port_scanning',
                            affected_target=f"{current_host or 'unknown'}:{port}",
                            port=port,
                            protocol=protocol,
                            evidence=f"Service: {service}, Version: {version}" if version else f"Service: {service}",
                            raw_data={
                                'tool': 'rustscan',
                                'host': current_host,
                                'port': port,
                                'protocol': protocol,
                                'state': state,
                                'service': service,
                                'version': version
                            }
                        )
                        findings.append(finding)
                
                # Parsear formato grepeable: Host: IP (up) Ports: 22/open/tcp//ssh///
                elif line.startswith('Host:'):
                    match = re.search(r'Host:\s+([\d\.]+).*Ports:\s+(.+)', line)
                    if match:
                        host = match.group(1)
                        ports_str = match.group(2)
                        
                        # Parsear cada puerto: 22/open/tcp//ssh///
                        port_entries = ports_str.split(',')
                        for entry in port_entries:
                            parts = entry.strip().split('/')
                            if len(parts) >= 6:
                                port = int(parts[0])
                                state = parts[1]
                                protocol = parts[2]
                                service = parts[4] if parts[4] else 'unknown'
                                
                                if state == 'open':
                                    finding = ParsedFinding(
                                        title=f"Open port: {port}/{protocol} ({service})",
                                        severity='info',
                                        description=f"Port {port}/{protocol} is open running {service}",
                                        category='port_scanning',
                                        affected_target=f"{host}:{port}",
                                        port=port,
                                        protocol=protocol,
                                        evidence=f"Service: {service}",
                                        raw_data={
                                            'tool': 'rustscan',
                                            'host': host,
                                            'port': port,
                                            'protocol': protocol,
                                            'state': state,
                                            'service': service
                                        }
                                    )
                                    findings.append(finding)
            
            logger.info(f"RustScan: Parsed {len(findings)} open ports")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing RustScan file {file_path}: {e}")
            return findings
```

---

### 2. MasscanParser

**Archivo**: `services/reporting/parsers/scanning/masscan_parser.py`

```python
"""
Parser para Masscan - Fast port scanner.
Formato: JSON con estructura de IPs y puertos.
"""

from pathlib import Path
from typing import List
import logging
from ..base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class MasscanParser(BaseParser):
    """Parser para archivos JSON de Masscan."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'masscan' in filename and file_path.suffix == '.json'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo JSON de Masscan.
        
        Formato esperado:
        [
          {
            "ip": "192.168.1.1",
            "timestamp": 1702214400,
            "ports": [
              {"port": 22, "proto": "tcp"},
              {"port": 80, "proto": "tcp"}
            ]
          }
        ]
        """
        findings = []
        
        try:
            data = self._safe_parse_json(file_path)
            if not data or not isinstance(data, list):
                logger.warning(f"Invalid Masscan JSON format: {file_path}")
                return findings
            
            for host_entry in data:
                ip = host_entry.get('ip', 'unknown')
                ports = host_entry.get('ports', [])
                timestamp = host_entry.get('timestamp')
                
                for port_info in ports:
                    port = port_info.get('port', 0)
                    protocol = port_info.get('proto', 'tcp')
                    
                    finding = ParsedFinding(
                        title=f"Open port: {port}/{protocol}",
                        severity='info',
                        description=f"Port {port}/{protocol} is open on {ip}",
                        category='port_scanning',
                        affected_target=f"{ip}:{port}",
                        port=port,
                        protocol=protocol,
                        evidence=f"Discovered via Masscan at timestamp {timestamp}" if timestamp else None,
                        raw_data={
                            'tool': 'masscan',
                            'ip': ip,
                            'port': port,
                            'protocol': protocol,
                            'timestamp': timestamp
                        }
                    )
                    findings.append(finding)
            
            logger.info(f"Masscan: Parsed {len(findings)} open ports")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing Masscan file {file_path}: {e}")
            return findings
```

---

### 3. NaabuParser

**Archivo**: `services/reporting/parsers/scanning/naabu_parser.py`

```python
"""
Parser para Naabu - Fast port scanner.
Formato: TXT con formato simple IP:PUERTO por l√≠nea.
"""

from pathlib import Path
from typing import List
import logging
from ..base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class NaabuParser(BaseParser):
    """Parser para archivos de Naabu."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'naabu' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de Naabu.
        
        Formato esperado (TXT):
        192.168.1.1:22
        192.168.1.1:80
        192.168.1.1:443
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                if not line or ':' not in line:
                    continue
                
                # Formato: IP:PUERTO
                parts = line.split(':')
                if len(parts) == 2:
                    ip = parts[0].strip()
                    try:
                        port = int(parts[1].strip())
                    except ValueError:
                        continue
                    
                    finding = ParsedFinding(
                        title=f"Open port: {port}/tcp",
                        severity='info',
                        description=f"Port {port}/tcp is open on {ip}",
                        category='port_scanning',
                        affected_target=f"{ip}:{port}",
                        port=port,
                        protocol='tcp',
                        evidence="Discovered via Naabu fast scan",
                        raw_data={
                            'tool': 'naabu',
                            'ip': ip,
                            'port': port,
                            'protocol': 'tcp'
                        }
                    )
                    findings.append(finding)
            
            logger.info(f"Naabu: Parsed {len(findings)} open ports")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing Naabu file {file_path}: {e}")
            return findings
```

---

## üü° FASE 2: SMB & SSL ENUMERATION {#fase2}

### 4. Enum4linuxParser

**Archivo**: `services/reporting/parsers/enumeration/smb/enum4linux_parser.py`

```python
"""
Parser para Enum4linux - SMB enumeration tool.
Formato: TXT con secciones delimitadas por ===.
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class Enum4linuxParser(BaseParser):
    """Parser para archivos de Enum4linux."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'enum4linux' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de Enum4linux.
        
        Secciones:
        - Target Information
        - Share Enumeration
        - Password Policy
        - Groups
        - Users
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.split('\n')
            current_section = None
            target_ip = None
            
            for line in lines:
                line_stripped = line.strip()
                
                # Detectar target IP
                if 'Target' in line and not target_ip:
                    match = re.search(r'([\d\.]+)', line)
                    if match:
                        target_ip = match.group(1)
                
                # Detectar secciones
                if '===' in line:
                    if 'Share Enumeration' in line:
                        current_section = 'shares'
                    elif 'Password Policy' in line:
                        current_section = 'password_policy'
                    elif 'Groups' in line:
                        current_section = 'groups'
                    elif 'Users' in line:
                        current_section = 'users'
                    continue
                
                # Parsear shares: Share name       Type      Comment
                if current_section == 'shares' and line_stripped:
                    match = re.match(r'^([^\s]+)\s+(Disk|IPC|Printer)\s+(.+)', line_stripped)
                    if match and match.group(1) not in ['Share', 'ADMIN$', 'IPC$']:
                        share_name = match.group(1)
                        share_type = match.group(2)
                        comment = match.group(3)
                        
                        # Shares p√∫blicos son riesgo
                        if 'public' in comment.lower() or share_type == 'Disk':
                            severity = 'low'
                        else:
                            severity = 'info'
                        
                        finding = ParsedFinding(
                            title=f"SMB Share discovered: {share_name}",
                            severity=severity,
                            description=f"SMB share '{share_name}' ({share_type}): {comment}",
                            category='smb_enumeration',
                            affected_target=f"{target_ip or 'unknown'}",
                            evidence=f"Type: {share_type}, Comment: {comment}",
                            remediation="Review share permissions and restrict access if needed",
                            raw_data={
                                'tool': 'enum4linux',
                                'type': 'share',
                                'share_name': share_name,
                                'share_type': share_type,
                                'comment': comment,
                                'target': target_ip
                            }
                        )
                        findings.append(finding)
                
                # Parsear users: index: 0x1 RID: 0x3f2 acb: 0x00000010 Account: user1
                elif current_section == 'users' and 'Account:' in line_stripped:
                    match = re.search(r'Account:\s+(\S+)', line_stripped)
                    if match:
                        username = match.group(1)
                        
                        finding = ParsedFinding(
                            title=f"SMB User enumerated: {username}",
                            severity='info',
                            description=f"User account '{username}' enumerated via SMB",
                            category='smb_enumeration',
                            affected_target=f"{target_ip or 'unknown'}",
                            evidence=f"Username: {username}",
                            remediation="Review user accounts and disable unnecessary accounts",
                            raw_data={
                                'tool': 'enum4linux',
                                'type': 'user',
                                'username': username,
                                'target': target_ip
                            }
                        )
                        findings.append(finding)
                
                # Parsear groups: group:[Administrators] rid:[0x220]
                elif current_section == 'groups' and 'group:[' in line_stripped:
                    match = re.search(r'group:\[([^\]]+)\]', line_stripped)
                    if match:
                        group_name = match.group(1)
                        
                        finding = ParsedFinding(
                            title=f"SMB Group enumerated: {group_name}",
                            severity='info',
                            description=f"Group '{group_name}' enumerated via SMB",
                            category='smb_enumeration',
                            affected_target=f"{target_ip or 'unknown'}",
                            evidence=f"Group: {group_name}",
                            raw_data={
                                'tool': 'enum4linux',
                                'type': 'group',
                                'group_name': group_name,
                                'target': target_ip
                            }
                        )
                        findings.append(finding)
            
            logger.info(f"Enum4linux: Parsed {len(findings)} SMB findings")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing Enum4linux file {file_path}: {e}")
            return findings
```

---

### 5. SMBMapParser

**Archivo**: `services/reporting/parsers/enumeration/smb/smbmap_parser.py`

```python
"""
Parser para SMBMap - SMB share enumeration.
Formato: TXT con tabla de shares y permisos.
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class SMBMapParser(BaseParser):
    """Parser para archivos de SMBMap."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'smbmap' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de SMBMap.
        
        Formato:
        [+] IP: 192.168.1.100:445	Name: TARGET-PC
        	Disk		Permissions	Comment
        	----		-----------	-------
        	ADMIN$		READ, WRITE	Remote Admin
        	Shared		READ ONLY	Public Share
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.split('\n')
            current_ip = None
            in_table = False
            
            for line in lines:
                line_stripped = line.strip()
                
                # Extraer IP: [+] IP: 192.168.1.100:445
                if line_stripped.startswith('[+] IP:'):
                    match = re.search(r'IP:\s+([\d\.]+)', line_stripped)
                    if match:
                        current_ip = match.group(1)
                    in_table = True
                    continue
                
                # Saltar headers de tabla
                if 'Disk' in line and 'Permissions' in line:
                    continue
                if '----' in line:
                    continue
                
                # Parsear l√≠neas de shares si estamos en tabla
                if in_table and line_stripped and current_ip:
                    # Formato: ADMIN$		READ, WRITE	Remote Admin
                    parts = re.split(r'\s{2,}', line_stripped)
                    if len(parts) >= 2:
                        share_name = parts[0].strip()
                        permissions = parts[1].strip() if len(parts) > 1 else 'UNKNOWN'
                        comment = parts[2].strip() if len(parts) > 2 else ''
                        
                        # Ignorar shares de sistema
                        if share_name in ['Disk', 'IPC$']:
                            continue
                        
                        # Determinar severidad
                        if 'WRITE' in permissions.upper():
                            severity = 'medium'  # Escritura es riesgo
                        elif 'READ' in permissions.upper() and share_name not in ['ADMIN$', 'C$']:
                            severity = 'low'
                        else:
                            severity = 'info'
                        
                        finding = ParsedFinding(
                            title=f"SMB Share: {share_name} ({permissions})",
                            severity=severity,
                            description=f"Share '{share_name}' accessible with {permissions} permissions",
                            category='smb_enumeration',
                            affected_target=f"{current_ip}",
                            evidence=f"Permissions: {permissions}, Comment: {comment}",
                            remediation="Review share permissions and restrict access to authorized users only" if severity != 'info' else None,
                            raw_data={
                                'tool': 'smbmap',
                                'share_name': share_name,
                                'permissions': permissions,
                                'comment': comment,
                                'target': current_ip
                            }
                        )
                        findings.append(finding)
                
                # Nueva IP reinicia la tabla
                if line_stripped.startswith('[+] IP:') or not line_stripped:
                    in_table = False
            
            logger.info(f"SMBMap: Parsed {len(findings)} SMB shares")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing SMBMap file {file_path}: {e}")
            return findings
```

---

### 6. SMBClientParser

**Archivo**: `services/reporting/parsers/enumeration/smb/smbclient_parser.py`

```python
"""
Parser para SMBClient - SMB client tool.
Formato: TXT con output interactivo tipo 'ls'.
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class SMBClientParser(BaseParser):
    """Parser para archivos de SMBClient."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'smbclient' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de SMBClient.
        
        Formato:
        smb: \> ls
          file.txt       A     1024  Mon Jan  1 00:00:00 2024
          document.pdf   A    51200  Mon Jan  1 00:00:00 2024
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.split('\n')
            files_found = []
            
            for line in lines:
                line_stripped = line.strip()
                
                # Parsear l√≠neas de archivos: filename  A  size  date
                # Formato:   file.txt       A     1024  Mon Jan  1 00:00:00 2024
                match = re.match(r'^([^\s]+)\s+(A|D)\s+(\d+)\s+(.+)', line_stripped)
                if match:
                    filename = match.group(1)
                    file_type = match.group(2)  # A=archivo, D=directorio
                    size = int(match.group(3))
                    date = match.group(4)
                    
                    # Ignorar directorios . y ..
                    if filename in ['.', '..']:
                        continue
                    
                    files_found.append({
                        'filename': filename,
                        'type': file_type,
                        'size': size,
                        'date': date
                    })
            
            # Crear un finding general si se encontraron archivos
            if files_found:
                file_count = len([f for f in files_found if f['type'] == 'A'])
                dir_count = len([f for f in files_found if f['type'] == 'D'])
                
                finding = ParsedFinding(
                    title=f"SMB Share contents enumerated",
                    severity='info',
                    description=f"Successfully enumerated share contents: {file_count} files, {dir_count} directories",
                    category='smb_enumeration',
                    affected_target='smb_share',
                    evidence=f"Files: {', '.join([f['filename'] for f in files_found[:5]])}{'...' if len(files_found) > 5 else ''}",
                    raw_data={
                        'tool': 'smbclient',
                        'file_count': file_count,
                        'directory_count': dir_count,
                        'files': files_found
                    }
                )
                findings.append(finding)
            
            logger.info(f"SMBClient: Parsed {len(files_found)} files/directories")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing SMBClient file {file_path}: {e}")
            return findings
```

---

### 7. SSLScanParser

**Archivo**: `services/reporting/parsers/enumeration/ssl/sslscan_parser.py`

```python
"""
Parser para SSLScan - SSL/TLS scanner.
Formato: TXT con secciones de ciphers y certificado.
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class SSLScanParser(BaseParser):
    """Parser para archivos de SSLScan."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'sslscan' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de SSLScan.
        
        Secciones:
        - Supported Server Cipher(s)
        - SSL Certificate
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.split('\n')
            target = None
            weak_ciphers = []
            strong_ciphers = []
            cert_info = {}
            
            for line in lines:
                line_stripped = line.strip()
                
                # Extraer target
                if 'Testing SSL server' in line:
                    match = re.search(r'server\s+([\d\.]+)\s+on port\s+(\d+)', line)
                    if match:
                        target = f"{match.group(1)}:{match.group(2)}"
                
                # Parsear ciphers: Accepted  TLSv1.2  256 bits  ECDHE-RSA-AES256-GCM-SHA384
                if line_stripped.startswith('Accepted'):
                    parts = line_stripped.split()
                    if len(parts) >= 5:
                        protocol = parts[1]
                        bits = parts[2]
                        cipher = parts[4]
                        
                        # Identificar ciphers d√©biles
                        if 'SSLv' in protocol or 'TLSv1.0' in protocol or 'TLSv1.1' in protocol:
                            weak_ciphers.append(f"{protocol} {cipher}")
                        else:
                            strong_ciphers.append(f"{protocol} {cipher}")
                
                # Extraer info del certificado
                if 'Subject:' in line:
                    match = re.search(r'Subject:\s+(.+)', line)
                    if match:
                        cert_info['subject'] = match.group(1)
                elif 'Issuer:' in line:
                    match = re.search(r'Issuer:\s+(.+)', line)
                    if match:
                        cert_info['issuer'] = match.group(1)
                elif 'Not valid before:' in line:
                    match = re.search(r'Not valid before:\s+(.+)', line)
                    if match:
                        cert_info['valid_from'] = match.group(1)
                elif 'Not valid after:' in line:
                    match = re.search(r'Not valid after:\s+(.+)', line)
                    if match:
                        cert_info['valid_until'] = match.group(1)
            
            # Finding para ciphers d√©biles
            if weak_ciphers:
                finding = ParsedFinding(
                    title=f"Weak SSL/TLS ciphers supported",
                    severity='medium',
                    description=f"{len(weak_ciphers)} weak cipher suites are supported",
                    category='ssl_enumeration',
                    affected_target=target or 'unknown',
                    evidence=f"Weak ciphers: {', '.join(weak_ciphers[:3])}{'...' if len(weak_ciphers) > 3 else ''}",
                    remediation="Disable weak protocols (SSLv2, SSLv3, TLSv1.0, TLSv1.1) and weak cipher suites",
                    raw_data={
                        'tool': 'sslscan',
                        'type': 'weak_ciphers',
                        'weak_ciphers': weak_ciphers,
                        'target': target
                    }
                )
                findings.append(finding)
            
            # Finding para certificado
            if cert_info:
                finding = ParsedFinding(
                    title=f"SSL Certificate information",
                    severity='info',
                    description=f"Certificate for {cert_info.get('subject', 'unknown')}",
                    category='ssl_enumeration',
                    affected_target=target or 'unknown',
                    evidence=f"Issuer: {cert_info.get('issuer', 'N/A')}, Valid until: {cert_info.get('valid_until', 'N/A')}",
                    raw_data={
                        'tool': 'sslscan',
                        'type': 'certificate',
                        'certificate': cert_info,
                        'target': target
                    }
                )
                findings.append(finding)
            
            # Finding general con resumen
            if strong_ciphers or weak_ciphers:
                finding = ParsedFinding(
                    title=f"SSL/TLS Configuration",
                    severity='info',
                    description=f"SSL/TLS scan completed: {len(strong_ciphers)} strong ciphers, {len(weak_ciphers)} weak ciphers",
                    category='ssl_enumeration',
                    affected_target=target or 'unknown',
                    evidence=f"Total ciphers: {len(strong_ciphers) + len(weak_ciphers)}",
                    raw_data={
                        'tool': 'sslscan',
                        'strong_cipher_count': len(strong_ciphers),
                        'weak_cipher_count': len(weak_ciphers),
                        'target': target
                    }
                )
                findings.append(finding)
            
            logger.info(f"SSLScan: Parsed SSL/TLS configuration with {len(weak_ciphers)} weak ciphers")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing SSLScan file {file_path}: {e}")
            return findings
```

---

### 8. SSLyzeParser

**Archivo**: `services/reporting/parsers/enumeration/ssl/sslyze_parser.py`

```python
"""
Parser para SSLyze - Advanced SSL/TLS scanner.
Formato: TXT con secciones por protocolo TLS.
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class SSLyzeParser(BaseParser):
    """Parser para archivos de SSLyze."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'sslyze' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de SSLyze.
        
        Secciones:
        - TLS 1.3/1.2 Cipher Suites
        - Certificate Information
        - Session Renegotiation
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.split('\n')
            target = None
            current_protocol = None
            weak_ciphers = []
            cert_info = {}
            renegotiation_secure = False
            
            for line in lines:
                line_stripped = line.strip()
                
                # Extraer target
                if 'SCAN RESULTS FOR' in line:
                    match = re.search(r'FOR\s+([\d\.]+:\d+)', line)
                    if match:
                        target = match.group(1)
                
                # Detectar protocolo
                if '* TLS' in line:
                    match = re.search(r'\* TLS ([\d\.]+)', line)
                    if match:
                        current_protocol = f"TLS {match.group(1)}"
                
                # Parsear ciphers aceptados
                if line_stripped.startswith('TLS_') or line_stripped.startswith('SSL_'):
                    cipher = line_stripped.split()[0]
                    
                    # Identificar ciphers d√©biles
                    if any(weak in cipher.upper() for weak in ['DES', 'RC4', 'MD5', 'EXPORT', 'NULL']):
                        weak_ciphers.append(f"{current_protocol} {cipher}")
                
                # Certificate info
                if 'Common Name:' in line:
                    match = re.search(r'Common Name:\s+(.+)', line)
                    if match:
                        cert_info['common_name'] = match.group(1)
                elif 'Issuer:' in line:
                    match = re.search(r'Issuer:\s+(.+)', line)
                    if match:
                        cert_info['issuer'] = match.group(1)
                elif 'Not After:' in line:
                    match = re.search(r'Not After:\s+(.+)', line)
                    if match:
                        cert_info['expiry'] = match.group(1)
                elif 'Key Size:' in line:
                    match = re.search(r'Key Size:\s+(\d+)', line)
                    if match:
                        cert_info['key_size'] = int(match.group(1))
                
                # Renegotiation
                if 'Secure Renegotiation:' in line:
                    renegotiation_secure = 'Supported' in line
            
            # Finding para weak ciphers
            if weak_ciphers:
                finding = ParsedFinding(
                    title=f"Weak SSL/TLS ciphers detected",
                    severity='high',
                    description=f"{len(weak_ciphers)} weak cipher suites supported",
                    category='ssl_vulnerability',
                    affected_target=target or 'unknown',
                    evidence=f"Weak ciphers: {', '.join(weak_ciphers)}",
                    remediation="Disable weak cipher suites containing DES, RC4, MD5, EXPORT, or NULL",
                    cve_id="CVE-2015-0204" if any('EXPORT' in c for c in weak_ciphers) else None,
                    raw_data={
                        'tool': 'sslyze',
                        'weak_ciphers': weak_ciphers,
                        'target': target
                    }
                )
                findings.append(finding)
            
            # Finding para key size d√©bil
            if cert_info.get('key_size') and cert_info['key_size'] < 2048:
                finding = ParsedFinding(
                    title=f"Weak RSA key size: {cert_info['key_size']} bits",
                    severity='medium',
                    description=f"Certificate uses RSA key size of {cert_info['key_size']} bits (minimum recommended: 2048 bits)",
                    category='ssl_vulnerability',
                    affected_target=target or 'unknown',
                    evidence=f"Key size: {cert_info['key_size']} bits",
                    remediation="Replace certificate with one using at least 2048-bit RSA key",
                    raw_data={
                        'tool': 'sslyze',
                        'key_size': cert_info['key_size'],
                        'target': target
                    }
                )
                findings.append(finding)
            
            # Finding para insecure renegotiation
            if not renegotiation_secure:
                finding = ParsedFinding(
                    title=f"Insecure TLS renegotiation",
                    severity='medium',
                    description="Server does not support secure renegotiation",
                    category='ssl_vulnerability',
                    affected_target=target or 'unknown',
                    evidence="Secure renegotiation: Not supported",
                    remediation="Enable secure renegotiation on the server",
                    cve_id="CVE-2009-3555",
                    raw_data={
                        'tool': 'sslyze',
                        'secure_renegotiation': renegotiation_secure,
                        'target': target
                    }
                )
                findings.append(finding)
            
            logger.info(f"SSLyze: Parsed SSL/TLS scan with {len(findings)} findings")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing SSLyze file {file_path}: {e}")
            return findings
```

---

---

## üåê FASE 3: NETWORK SERVICES ENUMERATION {#fase3}

### 9. SSHAuditParser

**Archivo**: `services/reporting/parsers/enumeration/network/ssh_audit_parser.py`

```python
"""
Parser para SSH-Audit - SSH configuration auditor.
Formato: TXT con categor√≠as y niveles [info], [warn], [fail].
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class SSHAuditParser(BaseParser):
    """Parser para archivos de SSH-Audit."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'ssh-audit' in filename or 'ssh_audit' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de SSH-Audit.
        
        Formato:
        (kex) diffie-hellman-group1-sha1  -- [fail] removed (in server)
        (enc) 3des-cbc                     -- [fail] removed (in server)
        (mac) hmac-sha1                    -- [warn] using weak hashing algorithm
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.split('\n')
            target = None
            ssh_version = None
            
            for line in lines:
                line_stripped = line.strip()
                
                # Extraer versi√≥n SSH
                if 'banner:' in line_stripped.lower():
                    match = re.search(r'banner:\s+(.+)', line_stripped, re.IGNORECASE)
                    if match:
                        ssh_version = match.group(1)
                
                # Parsear l√≠neas con niveles: (cat) algorithm -- [level] message
                match = re.match(r'\((\w+)\)\s+([^\s]+)\s+--\s+\[(\w+)\]\s+(.+)', line_stripped)
                if match:
                    category = match.group(1)
                    algorithm = match.group(2)
                    level = match.group(3)
                    message = match.group(4)
                    
                    # Mapear categor√≠as
                    category_map = {
                        'kex': 'Key Exchange',
                        'key': 'Host Key',
                        'enc': 'Encryption',
                        'mac': 'MAC Algorithm',
                        'gen': 'General'
                    }
                    category_name = category_map.get(category, category)
                    
                    # Mapear niveles a severidad
                    if level == 'fail':
                        severity = 'high'
                    elif level == 'warn':
                        severity = 'medium'
                    else:
                        severity = 'info'
                    
                    # Solo crear findings para warn y fail
                    if level in ['warn', 'fail']:
                        finding = ParsedFinding(
                            title=f"SSH {category_name}: {algorithm}",
                            severity=severity,
                            description=f"{category_name} algorithm '{algorithm}': {message}",
                            category='ssh_configuration',
                            affected_target=target or 'ssh_server',
                            evidence=f"Algorithm: {algorithm}, Issue: {message}",
                            remediation=f"Disable {algorithm} and use modern alternatives",
                            raw_data={
                                'tool': 'ssh-audit',
                                'category': category,
                                'algorithm': algorithm,
                                'level': level,
                                'message': message,
                                'ssh_version': ssh_version
                            }
                        )
                        findings.append(finding)
            
            logger.info(f"SSH-Audit: Parsed {len(findings)} SSH configuration issues")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing SSH-Audit file {file_path}: {e}")
            return findings
```

---

### 10. SMTPEnumParser

**Archivo**: `services/reporting/parsers/enumeration/network/smtp_enum_parser.py`

```python
"""
Parser para SMTP User Enumeration.
Formato: TXT simple IP:PORT username STATUS.
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class SMTPEnumParser(BaseParser):
    """Parser para archivos de SMTP user enumeration."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'smtp' in filename and 'enum' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de SMTP enumeration.
        
        Formato:
        192.168.1.1:25     admin exists
        192.168.1.1:25     user exists
        192.168.1.1:25     guest doesn't exist
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.strip().split('\n')
            valid_users = []
            
            for line in lines:
                line_stripped = line.strip()
                
                # Formato: IP:PORT    USERNAME STATUS
                match = re.match(r'^([\d\.]+):(\d+)\s+(\S+)\s+(exists|doesn\'t exist)', line_stripped)
                if match:
                    ip = match.group(1)
                    port = int(match.group(2))
                    username = match.group(3)
                    exists = match.group(4) == 'exists'
                    
                    if exists:
                        valid_users.append(username)
            
            # Crear finding con usuarios v√°lidos encontrados
            if valid_users:
                finding = ParsedFinding(
                    title=f"SMTP User enumeration successful",
                    severity='medium',
                    description=f"{len(valid_users)} valid email users enumerated via SMTP",
                    category='smtp_enumeration',
                    affected_target=f"{ip}:{port}" if 'ip' in locals() else 'smtp_server',
                    evidence=f"Valid users: {', '.join(valid_users[:10])}{'...' if len(valid_users) > 10 else ''}",
                    remediation="Disable VRFY and EXPN commands on SMTP server to prevent user enumeration",
                    raw_data={
                        'tool': 'smtp-user-enum',
                        'valid_users': valid_users,
                        'user_count': len(valid_users)
                    }
                )
                findings.append(finding)
            
            logger.info(f"SMTP Enumeration: Found {len(valid_users)} valid users")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing SMTP enum file {file_path}: {e}")
            return findings
```

---

### 11. DNSZoneParser

**Archivo**: `services/reporting/parsers/enumeration/network/dns_zone_parser.py`

```python
"""
Parser para DNS Zone Transfer (dig).
Formato: TXT con registros DNS est√°ndar.
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class DNSZoneParser(BaseParser):
    """Parser para archivos de DNS zone transfer."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return ('dig' in filename or 'zone' in filename) and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de DNS zone transfer.
        
        Formato:
        example.com.  3600  IN  SOA  ns1.example.com. admin.example.com. (...)
        example.com.  3600  IN  A    192.168.1.1
        www.example.com. 3600 IN A   192.168.1.2
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.split('\n')
            domain = None
            records = []
            
            for line in lines:
                line_stripped = line.strip()
                
                # Saltar comentarios
                if line_stripped.startswith(';'):
                    # Extraer dominio del header
                    if 'DiG' in line and 'AXFR' in line:
                        match = re.search(r'DiG.*?\s+([\w\.\-]+)\s+AXFR', line)
                        if match:
                            domain = match.group(1)
                    continue
                
                # Parsear registros DNS: DOMAIN  TTL  CLASS  TYPE  VALUE
                match = re.match(r'^([\w\.\-]+)\.\s+(\d+)\s+IN\s+(A|AAAA|CNAME|MX|NS|TXT|SOA)\s+(.+)', line_stripped)
                if match:
                    record_domain = match.group(1)
                    ttl = int(match.group(2))
                    record_type = match.group(3)
                    value = match.group(4)
                    
                    records.append({
                        'domain': record_domain,
                        'type': record_type,
                        'value': value,
                        'ttl': ttl
                    })
            
            # Finding general para zone transfer exitoso
            if records:
                finding = ParsedFinding(
                    title=f"DNS Zone Transfer successful",
                    severity='high',
                    description=f"DNS zone transfer allowed for domain {domain or 'unknown'} - {len(records)} records leaked",
                    category='dns_misconfiguration',
                    affected_target=domain or 'dns_server',
                    evidence=f"Records leaked: {len(records)} ({len([r for r in records if r['type'] == 'A'])} A, {len([r for r in records if r['type'] == 'MX'])} MX, {len([r for r in records if r['type'] == 'NS'])} NS)",
                    remediation="Restrict zone transfers (AXFR) to authorized secondary name servers only",
                    cve_id="CWE-284",
                    raw_data={
                        'tool': 'dig_zone_transfer',
                        'domain': domain,
                        'record_count': len(records),
                        'records': records[:50]  # Limitar a 50
                    }
                )
                findings.append(finding)
                
                # Findings adicionales por subdominios
                subdomains = [r['domain'] for r in records if r['type'] in ['A', 'AAAA', 'CNAME']]
                if subdomains:
                    finding = ParsedFinding(
                        title=f"Subdomains leaked via zone transfer",
                        severity='medium',
                        description=f"{len(subdomains)} subdomains discovered through DNS zone transfer",
                        category='information_disclosure',
                        affected_target=domain or 'dns_server',
                        evidence=f"Subdomains: {', '.join(subdomains[:10])}{'...' if len(subdomains) > 10 else ''}",
                        remediation="Restrict zone transfers to prevent information disclosure",
                        raw_data={
                            'tool': 'dig_zone_transfer',
                            'subdomains': subdomains
                        }
                    )
                    findings.append(finding)
            
            logger.info(f"DNS Zone Transfer: Parsed {len(records)} DNS records")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing DNS zone file {file_path}: {e}")
            return findings
```

---

### 12. SNMPWalkParser

**Archivo**: `services/reporting/parsers/enumeration/network/snmpwalk_parser.py`

```python
"""
Parser para SNMPWalk - SNMP enumeration.
Formato: TXT con OIDs y valores.
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class SNMPWalkParser(BaseParser):
    """Parser para archivos de SNMPWalk."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'snmpwalk' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de SNMPWalk.
        
        Formato:
        SNMPv2-MIB::sysDescr.0 = STRING: Linux server 5.4.0
        SNMPv2-MIB::sysContact.0 = STRING: admin@example.com
        IF-MIB::ifDescr.1 = STRING: eth0
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.split('\n')
            system_info = {}
            interfaces = []
            sensitive_info = []
            
            for line in lines:
                line_stripped = line.strip()
                if not line_stripped:
                    continue
                
                # Parsear formato: OID = TYPE: VALUE
                match = re.match(r'^([^\s]+)\s+=\s+(\w+):\s+(.+)', line_stripped)
                if match:
                    oid = match.group(1)
                    value_type = match.group(2)
                    value = match.group(3)
                    
                    # Extraer info del sistema
                    if 'sysDescr' in oid:
                        system_info['description'] = value
                    elif 'sysContact' in oid:
                        system_info['contact'] = value
                        if '@' in value:
                            sensitive_info.append(('email', value))
                    elif 'sysName' in oid:
                        system_info['hostname'] = value
                    elif 'sysLocation' in oid:
                        system_info['location'] = value
                    elif 'ifDescr' in oid:
                        interfaces.append(value)
            
            # Finding para info del sistema
            if system_info:
                finding = ParsedFinding(
                    title=f"SNMP system information disclosed",
                    severity='low',
                    description=f"System information accessible via SNMP: {system_info.get('hostname', 'unknown')}",
                    category='snmp_enumeration',
                    affected_target=system_info.get('hostname', 'snmp_host'),
                    evidence=f"Description: {system_info.get('description', 'N/A')}, Contact: {system_info.get('contact', 'N/A')}",
                    remediation="Restrict SNMP access and use SNMPv3 with authentication",
                    raw_data={
                        'tool': 'snmpwalk',
                        'system_info': system_info,
                        'interface_count': len(interfaces)
                    }
                )
                findings.append(finding)
            
            # Finding para info sensible
            if sensitive_info:
                finding = ParsedFinding(
                    title=f"Sensitive information in SNMP",
                    severity='medium',
                    description=f"Sensitive information exposed via SNMP (emails, contacts)",
                    category='information_disclosure',
                    affected_target=system_info.get('hostname', 'snmp_host'),
                    evidence=f"Sensitive data: {', '.join([f'{t}: {v}' for t, v in sensitive_info])}",
                    remediation="Remove sensitive information from SNMP strings and restrict access",
                    raw_data={
                        'tool': 'snmpwalk',
                        'sensitive_info': sensitive_info
                    }
                )
                findings.append(finding)
            
            logger.info(f"SNMPWalk: Parsed SNMP enumeration with {len(findings)} findings")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing SNMPWalk file {file_path}: {e}")
            return findings
```

---

### 13. OneSixtyOneParser

**Archivo**: `services/reporting/parsers/enumeration/network/onesixtyone_parser.py`

```python
"""
Parser para OneSixtyOne - SNMP community string brute force.
Formato: TXT con IP [COMMUNITY] SYSTEM_INFO.
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class OneSixtyOneParser(BaseParser):
    """Parser para archivos de OneSixtyOne."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'onesixtyone' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de OneSixtyOne.
        
        Formato:
        192.168.1.1 [public] Linux server 5.4.0
        192.168.1.2 [private] Windows Server 2019
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.strip().split('\n')
            
            for line in lines:
                line_stripped = line.strip()
                if not line_stripped:
                    continue
                
                # Formato: IP [COMMUNITY] SYSTEM_INFO
                match = re.match(r'^([\d\.]+)\s+\[([^\]]+)\]\s+(.+)', line_stripped)
                if match:
                    ip = match.group(1)
                    community = match.group(2)
                    system_info = match.group(3)
                    
                    # Severidad seg√∫n community string
                    if community.lower() in ['public', 'private']:
                        severity = 'high'
                    else:
                        severity = 'medium'
                    
                    finding = ParsedFinding(
                        title=f"SNMP community string discovered: {community}",
                        severity=severity,
                        description=f"Valid SNMP community string '{community}' found on {ip}",
                        category='snmp_misconfiguration',
                        affected_target=ip,
                        evidence=f"Community: {community}, System: {system_info}",
                        remediation="Change default SNMP community strings and use SNMPv3 with strong authentication",
                        raw_data={
                            'tool': 'onesixtyone',
                            'ip': ip,
                            'community_string': community,
                            'system_info': system_info,
                            'is_default': community.lower() in ['public', 'private']
                        }
                    )
                    findings.append(finding)
            
            logger.info(f"OneSixtyOne: Found {len(findings)} valid SNMP community strings")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing OneSixtyOne file {file_path}: {e}")
            return findings
```

---

### 14. LDAPSearchParser

**Archivo**: `services/reporting/parsers/enumeration/network/ldapsearch_parser.py`

```python
"""
Parser para LDAPSearch - LDAP enumeration.
Formato: TXT en formato LDIF (LDAP Data Interchange Format).
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class LDAPSearchParser(BaseParser):
    """Parser para archivos de LDAPSearch (formato LDIF)."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'ldapsearch' in filename or 'ldap' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo LDIF de LDAPSearch.
        
        Formato:
        dn: cn=john.doe,ou=users,dc=example,dc=com
        objectClass: person
        cn: john.doe
        mail: john.doe@example.com
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            # Dividir en entradas (separadas por l√≠nea en blanco)
            entries = re.split(r'\n\s*\n', content)
            
            users = []
            groups = []
            emails = []
            
            for entry in entries:
                lines = entry.strip().split('\n')
                if not lines:
                    continue
                
                entry_data = {}
                dn = None
                
                for line in lines:
                    line_stripped = line.strip()
                    if not line_stripped or line_stripped.startswith('#'):
                        continue
                    
                    # Parsear atributo: valor
                    if ':' in line_stripped:
                        parts = line_stripped.split(':', 1)
                        attr = parts[0].strip().lower()
                        value = parts[1].strip()
                        
                        if attr == 'dn':
                            dn = value
                        else:
                            entry_data[attr] = value
                
                # Clasificar entradas
                object_class = entry_data.get('objectclass', '').lower()
                
                if 'person' in object_class or 'inetorgperson' in object_class:
                    cn = entry_data.get('cn', 'unknown')
                    mail = entry_data.get('mail')
                    users.append({'cn': cn, 'dn': dn, 'mail': mail})
                    if mail:
                        emails.append(mail)
                
                elif 'group' in object_class or 'organizationalunit' in object_class:
                    ou = entry_data.get('ou') or entry_data.get('cn', 'unknown')
                    groups.append({'name': ou, 'dn': dn})
            
            # Finding para usuarios
            if users:
                finding = ParsedFinding(
                    title=f"LDAP users enumerated",
                    severity='medium',
                    description=f"{len(users)} user accounts enumerated via LDAP",
                    category='ldap_enumeration',
                    affected_target='ldap_server',
                    evidence=f"Users: {', '.join([u['cn'] for u in users[:10]])}{'...' if len(users) > 10 else ''}",
                    remediation="Restrict anonymous LDAP binds and implement proper access controls",
                    raw_data={
                        'tool': 'ldapsearch',
                        'users': users,
                        'user_count': len(users)
                    }
                )
                findings.append(finding)
            
            # Finding para emails
            if emails:
                finding = ParsedFinding(
                    title=f"Email addresses exposed via LDAP",
                    severity='low',
                    description=f"{len(emails)} email addresses discovered",
                    category='information_disclosure',
                    affected_target='ldap_server',
                    evidence=f"Emails: {', '.join(emails[:5])}{'...' if len(emails) > 5 else ''}",
                    remediation="Restrict LDAP queries or remove email addresses from public attributes",
                    raw_data={
                        'tool': 'ldapsearch',
                        'emails': emails
                    }
                )
                findings.append(finding)
            
            # Finding para grupos
            if groups:
                finding = ParsedFinding(
                    title=f"LDAP groups/OUs enumerated",
                    severity='info',
                    description=f"{len(groups)} organizational units/groups discovered",
                    category='ldap_enumeration',
                    affected_target='ldap_server',
                    evidence=f"Groups: {', '.join([g['name'] for g in groups[:10]])}{'...' if len(groups) > 10 else ''}",
                    raw_data={
                        'tool': 'ldapsearch',
                        'groups': groups
                    }
                )
                findings.append(finding)
            
            logger.info(f"LDAPSearch: Parsed {len(users)} users, {len(groups)} groups")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing LDAPSearch file {file_path}: {e}")
            return findings
```

---

## üíæ FASE 4: DATABASE ENUMERATION {#fase4}

### 15. MySQLEnumParser

**Archivo**: `services/reporting/parsers/enumeration/database/mysql_enum_parser.py`

```python
"""
Parser para MySQL Enumeration.
Formato: TXT con secciones de version, databases, users, tables.
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class MySQLEnumParser(BaseParser):
    """Parser para archivos de MySQL enumeration."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'mysql' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de MySQL enumeration.
        
        Secciones:
        - MySQL version
        - Databases
        - Users
        - Tables
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.split('\n')
            mysql_version = None
            databases = []
            users = []
            current_section = None
            
            for line in lines:
                line_stripped = line.strip()
                if not line_stripped:
                    continue
                
                # Extraer versi√≥n
                if 'MySQL version:' in line or 'Server version:' in line:
                    match = re.search(r'(\d+\.\d+\.\d+)', line)
                    if match:
                        mysql_version = match.group(1)
                    continue
                
                # Detectar secciones
                if 'Databases:' in line:
                    current_section = 'databases'
                    continue
                elif 'Users:' in line:
                    current_section = 'users'
                    continue
                elif 'Tables' in line:
                    current_section = 'tables'
                    continue
                
                # Parsear databases
                if current_section == 'databases':
                    if not line_stripped.startswith('information_schema'):
                        databases.append(line_stripped)
                
                # Parsear users
                elif current_section == 'users':
                    if '@' in line_stripped:
                        users.append(line_stripped)
            
            # Finding para databases
            if databases:
                user_databases = [db for db in databases if db not in ['information_schema', 'mysql', 'performance_schema', 'sys']]
                
                if user_databases:
                    finding = ParsedFinding(
                        title=f"MySQL databases enumerated",
                        severity='info',
                        description=f"{len(user_databases)} user databases discovered",
                        category='database_enumeration',
                        affected_target='mysql_server',
                        evidence=f"Databases: {', '.join(user_databases[:10])}{'...' if len(user_databases) > 10 else ''}",
                        raw_data={
                            'tool': 'mysql_enum',
                            'version': mysql_version,
                            'databases': user_databases
                        }
                    )
                    findings.append(finding)
            
            # Finding para usuarios remotos
            if users:
                remote_users = [u for u in users if '%' in u or not u.endswith('@localhost')]
                
                if remote_users:
                    finding = ParsedFinding(
                        title=f"MySQL remote users detected",
                        severity='medium',
                        description=f"{len(remote_users)} MySQL users with remote access",
                        category='database_misconfiguration',
                        affected_target='mysql_server',
                        evidence=f"Remote users: {', '.join(remote_users[:5])}{'...' if len(remote_users) > 5 else ''}",
                        remediation="Review and restrict MySQL user access to localhost where possible",
                        raw_data={
                            'tool': 'mysql_enum',
                            'users': users,
                            'remote_users': remote_users
                        }
                    )
                    findings.append(finding)
            
            logger.info(f"MySQL Enum: Parsed {len(databases)} databases, {len(users)} users")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing MySQL enum file {file_path}: {e}")
            return findings
```

---

### 16. PostgreSQLEnumParser

**Archivo**: `services/reporting/parsers/enumeration/database/postgresql_enum_parser.py`

```python
"""
Parser para PostgreSQL Enumeration.
Formato: TXT con secciones de version, databases, users/roles, schemas.
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class PostgreSQLEnumParser(BaseParser):
    """Parser para archivos de PostgreSQL enumeration."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return ('psql' in filename or 'postgresql' in filename or 'postgres' in filename) and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de PostgreSQL enumeration.
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.split('\n')
            pg_version = None
            databases = []
            superusers = []
            current_section = None
            
            for line in lines:
                line_stripped = line.strip()
                if not line_stripped:
                    continue
                
                # Extraer versi√≥n
                if 'PostgreSQL version:' in line or 'Server version:' in line:
                    match = re.search(r'(\d+\.\d+)', line)
                    if match:
                        pg_version = match.group(1)
                    continue
                
                # Detectar secciones
                if 'Databases:' in line:
                    current_section = 'databases'
                    continue
                elif 'Users/Roles:' in line:
                    current_section = 'users'
                    continue
                elif 'Schemas' in line:
                    current_section = 'schemas'
                    continue
                
                # Parsear databases
                if current_section == 'databases':
                    if not line_stripped.startswith('template') and line_stripped != 'postgres':
                        databases.append(line_stripped)
                
                # Parsear users/roles
                elif current_section == 'users':
                    if '(Superuser)' in line_stripped:
                        match = re.match(r'^(\w+)\s+\(Superuser\)', line_stripped)
                        if match:
                            superusers.append(match.group(1))
            
            # Finding para databases
            if databases:
                finding = ParsedFinding(
                    title=f"PostgreSQL databases enumerated",
                    severity='info',
                    description=f"{len(databases)} user databases discovered",
                    category='database_enumeration',
                    affected_target='postgresql_server',
                    evidence=f"Databases: {', '.join(databases[:10])}{'...' if len(databases) > 10 else ''}",
                    raw_data={
                        'tool': 'postgresql_enum',
                        'version': pg_version,
                        'databases': databases
                    }
                )
                findings.append(finding)
            
            # Finding para superusers
            if superusers:
                finding = ParsedFinding(
                    title=f"PostgreSQL superusers detected",
                    severity='low',
                    description=f"{len(superusers)} PostgreSQL superuser accounts found",
                    category='database_enumeration',
                    affected_target='postgresql_server',
                    evidence=f"Superusers: {', '.join(superusers)}",
                    remediation="Review superuser accounts and grant privileges following principle of least privilege",
                    raw_data={
                        'tool': 'postgresql_enum',
                        'superusers': superusers
                    }
                )
                findings.append(finding)
            
            logger.info(f"PostgreSQL Enum: Parsed {len(databases)} databases, {len(superusers)} superusers")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing PostgreSQL enum file {file_path}: {e}")
            return findings
```

---

### 17. RedisEnumParser

**Archivo**: `services/reporting/parsers/enumeration/database/redis_enum_parser.py`

```python
"""
Parser para Redis Enumeration.
Formato: TXT con info de Redis (version, keys, memory).
"""

from pathlib import Path
from typing import List
import re
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class RedisEnumParser(BaseParser):
    """Parser para archivos de Redis enumeration."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'redis' in filename and file_path.suffix == '.txt'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo de Redis enumeration.
        
        Formato:
        Redis version: 6.2.6
        Keys: 100
        Memory: 1048576 bytes
        """
        findings = []
        
        try:
            content = self._read_file(file_path)
            if not content:
                return findings
            
            lines = content.split('\n')
            redis_version = None
            key_count = 0
            memory_bytes = 0
            
            for line in lines:
                line_stripped = line.strip()
                
                # Extraer versi√≥n
                if 'Redis version:' in line:
                    match = re.search(r'(\d+\.\d+\.\d+)', line)
                    if match:
                        redis_version = match.group(1)
                
                # Extraer key count
                elif 'Keys:' in line:
                    match = re.search(r'Keys:\s+(\d+)', line)
                    if match:
                        key_count = int(match.group(1))
                
                # Extraer memory
                elif 'Memory:' in line:
                    match = re.search(r'Memory:\s+(\d+)', line)
                    if match:
                        memory_bytes = int(match.group(1))
            
            # Finding para Redis accesible
            if redis_version or key_count > 0:
                severity = 'high' if key_count > 0 else 'medium'
                
                finding = ParsedFinding(
                    title=f"Redis instance accessible",
                    severity=severity,
                    description=f"Redis {redis_version or 'unknown version'} accessible with {key_count} keys",
                    category='database_misconfiguration',
                    affected_target='redis_server',
                    evidence=f"Version: {redis_version}, Keys: {key_count}, Memory: {memory_bytes} bytes",
                    remediation="Enable Redis authentication (requirepass) and bind to localhost only",
                    raw_data={
                        'tool': 'redis_enum',
                        'version': redis_version,
                        'key_count': key_count,
                        'memory_bytes': memory_bytes
                    }
                )
                findings.append(finding)
            
            logger.info(f"Redis Enum: Parsed Redis instance with {key_count} keys")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing Redis enum file {file_path}: {e}")
            return findings
```

---

## üî¥ FASE 5: VULNERABILITY ASSESSMENT {#fase5}

### 18. SQLMapParser

**Archivo**: `services/reporting/parsers/vulnerability/sqlmap_parser.py`

```python
"""
Parser para SQLMap - SQL injection scanner.
Formato: CSV dentro de directorio results-*.csv.
"""

from pathlib import Path
from typing import List
import csv
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class SQLMapParser(BaseParser):
    """Parser para archivos CSV de SQLMap."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        if file_path.is_dir() and 'sqlmap' in file_path.name.lower():
            return True
        filename = file_path.name.lower()
        return 'sqlmap' in filename and file_path.suffix == '.csv'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivos CSV de SQLMap.
        
        Formato CSV:
        id,title,payload,where,vector,parameter
        1,"SQL injection","1' OR '1'='1","GET","boolean-based blind","id"
        """
        findings = []
        
        try:
            # Si es directorio, buscar CSV dentro
            csv_files = []
            if file_path.is_dir():
                csv_files = list(file_path.glob('results-*.csv'))
            else:
                csv_files = [file_path]
            
            for csv_file in csv_files:
                try:
                    with open(csv_file, 'r', encoding='utf-8') as f:
                        reader = csv.DictReader(f)
                        
                        for row in reader:
                            title = row.get('title', 'SQL Injection')
                            payload = row.get('payload', '')
                            parameter = row.get('parameter', 'unknown')
                            vector = row.get('vector', 'unknown')
                            where = row.get('where', 'unknown')
                            
                            finding = ParsedFinding(
                                title=f"SQL Injection: {parameter}",
                                severity='critical',
                                description=f"SQL injection vulnerability found in parameter '{parameter}' ({vector})",
                                category='sql_injection',
                                affected_target=parameter,
                                evidence=f"Payload: {payload[:100]}{'...' if len(payload) > 100 else ''}, Vector: {vector}",
                                remediation="Use parameterized queries (prepared statements) and input validation",
                                cve_id="CWE-89",
                                raw_data={
                                    'tool': 'sqlmap',
                                    'title': title,
                                    'parameter': parameter,
                                    'payload': payload,
                                    'vector': vector,
                                    'where': where
                                }
                            )
                            findings.append(finding)
                
                except Exception as e:
                    logger.error(f"Error parsing SQLMap CSV file {csv_file}: {e}")
                    continue
            
            logger.info(f"SQLMap: Parsed {len(findings)} SQL injection findings")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing SQLMap file {file_path}: {e}")
            return findings
```

---

### 19. OWASPZAPParser

**Archivo**: `services/reporting/parsers/vulnerability/owasp_zap_parser.py`

```python
"""
Parser para OWASP ZAP - Web vulnerability scanner.
Formato: JSON con alerts.
"""

from pathlib import Path
from typing import List
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class OWASPZAPParser(BaseParser):
    """Parser para archivos JSON de OWASP ZAP."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'zap' in filename and file_path.suffix == '.json'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo JSON de OWASP ZAP.
        """
        findings = []
        
        try:
            data = self._safe_parse_json(file_path)
            if not data:
                return findings
            
            sites = data.get('site', [])
            
            for site in sites:
                site_name = site.get('@name', 'unknown')
                alerts = site.get('alerts', [])
                
                for alert in alerts:
                    alert_name = alert.get('alert', 'Unknown')
                    risk_code = int(alert.get('riskcode', '0'))
                    confidence = int(alert.get('confidence', '0'))
                    risk_desc = alert.get('riskdesc', '')
                    description = alert.get('desc', '')
                    
                    # Mapear risk code a severidad
                    severity_map = {
                        3: 'high',
                        2: 'medium',
                        1: 'low',
                        0: 'info'
                    }
                    severity = severity_map.get(risk_code, 'info')
                    
                    finding = ParsedFinding(
                        title=f"ZAP: {alert_name}",
                        severity=severity,
                        description=description if description else alert_name,
                        category='web_vulnerability',
                        affected_target=site_name,
                        evidence=f"Risk: {risk_desc}, Confidence: {confidence}",
                        remediation=alert.get('solution', 'Review and fix the identified issue'),
                        references=[alert.get('reference')] if alert.get('reference') else None,
                        raw_data={
                            'tool': 'owasp_zap',
                            'alert': alert_name,
                            'risk_code': risk_code,
                            'confidence': confidence,
                            'plugin_id': alert.get('pluginid')
                        }
                    )
                    findings.append(finding)
            
            logger.info(f"OWASP ZAP: Parsed {len(findings)} vulnerabilities")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing OWASP ZAP file {file_path}: {e}")
            return findings
```

---

### 20. WPScanParser

**Archivo**: `services/reporting/parsers/vulnerability/wpscan_parser.py`

```python
"""
Parser para WPScan - WordPress security scanner.
Formato: JSON con versi√≥n, plugins y vulnerabilidades.
"""

from pathlib import Path
from typing import List
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class WPScanParser(BaseParser):
    """Parser para archivos JSON de WPScan."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'wpscan' in filename and file_path.suffix == '.json'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo JSON de WPScan.
        """
        findings = []
        
        try:
            data = self._safe_parse_json(file_path)
            if not data:
                return findings
            
            url = data.get('url', 'unknown')
            
            # Parsear versi√≥n de WordPress
            version_info = data.get('version', {})
            wp_version = version_info.get('number', 'unknown')
            version_vulns = version_info.get('vulnerabilities', [])
            
            for vuln in version_vulns:
                title = vuln.get('title', 'WordPress Core Vulnerability')
                fixed_in = vuln.get('fixed_in')
                
                finding = ParsedFinding(
                    title=f"WordPress: {title}",
                    severity='high',
                    description=f"Vulnerability in WordPress {wp_version}: {title}",
                    category='cms_vulnerability',
                    affected_target=url,
                    evidence=f"Current version: {wp_version}, Fixed in: {fixed_in or 'N/A'}",
                    remediation=f"Update WordPress to version {fixed_in}" if fixed_in else "Update WordPress to latest version",
                    references=[vuln.get('references', {}).get('url', [])] if vuln.get('references') else None,
                    raw_data={
                        'tool': 'wpscan',
                        'type': 'core_vulnerability',
                        'version': wp_version,
                        'fixed_in': fixed_in
                    }
                )
                findings.append(finding)
            
            # Parsear plugins
            plugins = data.get('plugins', {})
            for plugin_name, plugin_data in plugins.items():
                plugin_version = plugin_data.get('version', {}).get('number', 'unknown')
                plugin_vulns = plugin_data.get('vulnerabilities', [])
                
                for vuln in plugin_vulns:
                    title = vuln.get('title', 'Plugin Vulnerability')
                    fixed_in = vuln.get('fixed_in')
                    
                    finding = ParsedFinding(
                        title=f"WordPress Plugin: {plugin_name} - {title}",
                        severity='medium',
                        description=f"Vulnerability in plugin {plugin_name} v{plugin_version}",
                        category='cms_vulnerability',
                        affected_target=url,
                        evidence=f"Plugin: {plugin_name}, Version: {plugin_version}, Fixed in: {fixed_in or 'N/A'}",
                        remediation=f"Update {plugin_name} to version {fixed_in}" if fixed_in else f"Update or remove {plugin_name}",
                        raw_data={
                            'tool': 'wpscan',
                            'type': 'plugin_vulnerability',
                            'plugin': plugin_name,
                            'version': plugin_version,
                            'fixed_in': fixed_in
                        }
                    )
                    findings.append(finding)
            
            logger.info(f"WPScan: Parsed {len(findings)} WordPress vulnerabilities")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing WPScan file {file_path}: {e}")
            return findings
```

---

### 21. TestSSLParser

**Archivo**: `services/reporting/parsers/enumeration/ssl/testssl_parser.py`

```python
"""
Parser para testssl.sh - Comprehensive SSL/TLS scanner.
Formato: JSON con protocols, ciphers, vulnerabilities.
"""

from pathlib import Path
from typing import List
import logging
from ...base_parser import BaseParser, ParsedFinding

logger = logging.getLogger(__name__)

class TestSSLParser(BaseParser):
    """Parser para archivos JSON de testssl.sh."""
    
    def can_parse(self, file_path: Path) -> bool:
        """Verifica si el archivo puede ser parseado."""
        filename = file_path.name.lower()
        return 'testssl' in filename and file_path.suffix == '.json'
    
    def parse(self, file_path: Path) -> List[ParsedFinding]:
        """
        Parsea archivo JSON de testssl.sh.
        """
        findings = []
        
        try:
            data = self._safe_parse_json(file_path)
            if not data:
                return findings
            
            target = data.get('target', 'unknown')
            protocols = data.get('protocols', {})
            ciphers = data.get('ciphers', [])
            
            # Check for weak protocols
            weak_protocols = []
            for protocol, status in protocols.items():
                if status == 'offered':
                    if any(weak in protocol for weak in ['SSLv2', 'SSLv3', 'TLSv1.0', 'TLSv1.1']):
                        weak_protocols.append(protocol)
            
            if weak_protocols:
                finding = ParsedFinding(
                    title=f"Weak SSL/TLS protocols supported",
                    severity='high',
                    description=f"Server supports weak SSL/TLS protocols: {', '.join(weak_protocols)}",
                    category='ssl_vulnerability',
                    affected_target=target,
                    evidence=f"Weak protocols: {', '.join(weak_protocols)}",
                    remediation="Disable SSLv2, SSLv3, TLSv1.0, and TLSv1.1. Use TLSv1.2 and TLSv1.3 only",
                    raw_data={
                        'tool': 'testssl',
                        'weak_protocols': weak_protocols,
                        'all_protocols': protocols
                    }
                )
                findings.append(finding)
            
            # Check for weak ciphers
            weak_ciphers = []
            for cipher in ciphers:
                cipher_name = cipher.get('cipher', '')
                if any(weak in cipher_name.upper() for weak in ['DES', 'RC4', 'MD5', 'NULL', 'EXPORT']):
                    weak_ciphers.append(cipher_name)
            
            if weak_ciphers:
                finding = ParsedFinding(
                    title=f"Weak cipher suites detected",
                    severity='medium',
                    description=f"{len(weak_ciphers)} weak cipher suites are supported",
                    category='ssl_vulnerability',
                    affected_target=target,
                    evidence=f"Weak ciphers: {', '.join(weak_ciphers[:5])}{'...' if len(weak_ciphers) > 5 else ''}",
                    remediation="Disable weak cipher suites and use strong encryption algorithms",
                    raw_data={
                        'tool': 'testssl',
                        'weak_ciphers': weak_ciphers
                    }
                )
                findings.append(finding)
            
            logger.info(f"testssl.sh: Parsed {len(findings)} SSL/TLS issues")
            return findings
            
        except Exception as e:
            logger.error(f"Error parsing testssl file {file_path}: {e}")
            return findings
```

---

## üß™ TESTS UNITARIOS {#tests}

### Archivo de Tests

**Ubicaci√≥n**: `tests/unit/test_scanning_parsers.py`

```python
"""
Tests unitarios para parsers de scanning y enumeration.
"""

import pytest
from pathlib import Path

# Importar todos los parsers
from services.reporting.parsers.scanning import (
    rustscan_parser, masscan_parser, naabu_parser
)
from services.reporting.parsers.enumeration.smb import (
    enum4linux_parser, smbmap_parser, smbclient_parser
)
from services.reporting.parsers.enumeration.ssl import (
    sslscan_parser, sslyze_parser, testssl_parser
)
from services.reporting.parsers.enumeration.network import (
    ssh_audit_parser, smtp_enum_parser, dns_zone_parser,
    snmpwalk_parser, onesixtyone_parser, ldapsearch_parser
)
from services.reporting.parsers.enumeration.database import (
    mysql_enum_parser, postgresql_enum_parser, redis_enum_parser
)
from services.reporting.parsers.vulnerability import (
    sqlmap_parser, owasp_zap_parser, wpscan_parser
)

FIXTURES_DIR = Path(__file__).parent.parent / 'fixtures' / 'scanning'


class TestPortScanningParsers:
    """Tests para parsers de port scanning."""
    
    def test_rustscan_parser(self):
        parser = rustscan_parser.RustScanParser()
        fixture_file = FIXTURES_DIR / 'rustscan_sample.txt'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) > 0
        assert all(f.category == 'port_scanning' for f in findings)
        assert all('port' in f.raw_data for f in findings)
    
    def test_masscan_parser(self):
        parser = masscan_parser.MasscanParser()
        fixture_file = FIXTURES_DIR / 'masscan_sample.json'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) > 0
        assert all(f.severity == 'info' for f in findings)
    
    def test_naabu_parser(self):
        parser = naabu_parser.NaabuParser()
        fixture_file = FIXTURES_DIR / 'naabu_sample.txt'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) > 0
        assert all(':' in f.affected_target for f in findings)


class TestSMBParsers:
    """Tests para parsers de SMB."""
    
    def test_enum4linux_parser(self):
        parser = enum4linux_parser.Enum4linuxParser()
        fixture_file = FIXTURES_DIR / 'enum4linux_sample.txt'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) > 0
        assert all(f.category == 'smb_enumeration' for f in findings)
    
    def test_smbmap_parser(self):
        parser = smbmap_parser.SMBMapParser()
        fixture_file = FIXTURES_DIR / 'smbmap_sample.txt'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) >= 0


class TestSSLParsers:
    """Tests para parsers de SSL."""
    
    def test_sslscan_parser(self):
        parser = sslscan_parser.SSLScanParser()
        fixture_file = FIXTURES_DIR / 'sslscan_sample.txt'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) > 0
        assert any('ssl' in f.category.lower() for f in findings)
    
    def test_sslyze_parser(self):
        parser = sslyze_parser.SSLyzeParser()
        fixture_file = FIXTURES_DIR / 'sslyze_sample.txt'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) >= 0


class TestNetworkServicesParsers:
    """Tests para parsers de servicios de red."""
    
    def test_ssh_audit_parser(self):
        parser = ssh_audit_parser.SSHAuditParser()
        fixture_file = FIXTURES_DIR / 'ssh_audit_sample.txt'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) >= 0
    
    def test_dns_zone_parser(self):
        parser = dns_zone_parser.DNSZoneParser()
        fixture_file = FIXTURES_DIR / 'dig_zone_sample.txt'
        
        findings = parser.parse(fixture_file)
        
        if len(findings) > 0:
            assert any('dns' in f.category.lower() for f in findings)


class TestDatabaseParsers:
    """Tests para parsers de bases de datos."""
    
    def test_mysql_enum_parser(self):
        parser = mysql_enum_parser.MySQLEnumParser()
        fixture_file = FIXTURES_DIR / 'mysql_sample.txt'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) >= 0
    
    def test_postgresql_enum_parser(self):
        parser = postgresql_enum_parser.PostgreSQLEnumParser()
        fixture_file = FIXTURES_DIR / 'psql_sample.txt'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) >= 0


class TestVulnerabilityParsers:
    """Tests para parsers de vulnerabilidades."""
    
    def test_sqlmap_parser(self):
        parser = sqlmap_parser.SQLMapParser()
        fixture_file = FIXTURES_DIR / 'sqlmap_sample.csv'
        
        findings = parser.parse(fixture_file)
        
        if len(findings) > 0:
            assert all(f.severity == 'critical' for f in findings)
    
    def test_owasp_zap_parser(self):
        parser = owasp_zap_parser.OWASPZAPParser()
        fixture_file = FIXTURES_DIR / 'zap_sample.json'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) >= 0
    
    def test_wpscan_parser(self):
        parser = wpscan_parser.WPScanParser()
        fixture_file = FIXTURES_DIR / 'wpscan_sample.json'
        
        findings = parser.parse(fixture_file)
        
        assert len(findings) >= 0
```

---

## üîó REGISTRO EN PARSERMANAGER {#registro}

### Actualizar ParserManager

**Archivo**: `services/reporting/parsers/parser_manager.py`

```python
# Port Scanning
from .scanning.rustscan_parser import RustScanParser
from .scanning.masscan_parser import MasscanParser
from .scanning.naabu_parser import NaabuParser

# SMB Enumeration
from .enumeration.smb.enum4linux_parser import Enum4linuxParser
from .enumeration.smb.smbmap_parser import SMBMapParser
from .enumeration.smb.smbclient_parser import SMBClientParser

# SSL Enumeration
from .enumeration.ssl.sslscan_parser import SSLScanParser
from .enumeration.ssl.sslyze_parser import SSLyzeParser
from .enumeration.ssl.testssl_parser import TestSSLParser

# Network Services
from .enumeration.network.ssh_audit_parser import SSHAuditParser
from .enumeration.network.smtp_enum_parser import SMTPEnumParser
from .enumeration.network.dns_zone_parser import DNSZoneParser
from .enumeration.network.snmpwalk_parser import SNMPWalkParser
from .enumeration.network.onesixtyone_parser import OneSixtyOneParser
from .enumeration.network.ldapsearch_parser import LDAPSearchParser

# Database Enumeration
from .enumeration.database.mysql_enum_parser import MySQLEnumParser
from .enumeration.database.postgresql_enum_parser import PostgreSQLEnumParser
from .enumeration.database.redis_enum_parser import RedisEnumParser

# Vulnerability Assessment
from .vulnerability.sqlmap_parser import SQLMapParser
from .vulnerability.owasp_zap_parser import OWASPZAPParser
from .vulnerability.wpscan_parser import WPScanParser


class ParserManager:
    def __init__(self):
        self.parsers = []
        self._register_parsers()
    
    def _register_parsers(self):
        # Parsers existentes
        self.parsers.extend([
            NmapParser(),
            NucleiParser(),
            NiktoParser(),
            SubfinderParser(),
            AmassParser(),
        ])
        
        # Port Scanning
        self.parsers.extend([
            RustScanParser(),
            MasscanParser(),
            NaabuParser(),
        ])
        
        # SMB Enumeration
        self.parsers.extend([
            Enum4linuxParser(),
            SMBMapParser(),
            SMBClientParser(),
        ])
        
        # SSL Enumeration
        self.parsers.extend([
            SSLScanParser(),
            SSLyzeParser(),
            TestSSLParser(),
        ])
        
        # Network Services
        self.parsers.extend([
            SSHAuditParser(),
            SMTPEnumParser(),
            DNSZoneParser(),
            SNMPWalkParser(),
            OneSixtyOneParser(),
            LDAPSearchParser(),
        ])
        
        # Database Enumeration
        self.parsers.extend([
            MySQLEnumParser(),
            PostgreSQLEnumParser(),
            RedisEnumParser(),
        ])
        
        # Vulnerability Assessment
        self.parsers.extend([
            SQLMapParser(),
            OWASPZAPParser(),
            WPScanParser(),
        ])
```

---

## ‚úÖ VALIDACI√ìN {#validacion}

### Checklist de Implementaci√≥n

**FASE 1: Port Scanning**
- [ ] RustScanParser implementado
- [ ] MasscanParser implementado
- [ ] NaabuParser implementado
- [ ] Tests passing
- [ ] Registrados en ParserManager

**FASE 2: SMB & SSL**
- [ ] Enum4linuxParser implementado
- [ ] SMBMapParser implementado
- [ ] SMBClientParser implementado
- [ ] SSLScanParser implementado
- [ ] SSLyzeParser implementado
- [ ] Tests passing

**FASE 3: Network Services**
- [ ] SSHAuditParser implementado
- [ ] SMTPEnumParser implementado
- [ ] DNSZoneParser implementado
- [ ] SNMPWalkParser implementado
- [ ] OneSixtyOneParser implementado
- [ ] LDAPSearchParser implementado
- [ ] Tests passing

**FASE 4: Databases**
- [ ] MySQLEnumParser implementado
- [ ] PostgreSQLEnumParser implementado
- [ ] RedisEnumParser implementado
- [ ] Tests passing

**FASE 5: Vulnerabilities**
- [ ] SQLMapParser implementado
- [ ] OWASPZAPParser implementado
- [ ] WPScanParser implementado
- [ ] TestSSLParser implementado
- [ ] Tests passing

### Validaci√≥n Final

```bash
# Ejecutar todos los tests
pytest tests/unit/test_scanning_parsers.py -v

# Verificar ParserManager
python -c "from services.reporting.parsers.parser_manager import ParserManager; pm = ParserManager(); print(f'Total: {len(pm.parsers)}')"

# Generar reporte de prueba
# Frontend ‚Üí Generate Report ‚Üí Verificar logs
```

---

## üéØ RESUMEN

**Parsers implementados**: 21 nuevos  
**Tiempo estimado**: 8-9 horas  
**Complejidad**: Media-Alta  
**Beneficio**: Cobertura completa de scanning y enumeration

**Resultado**: Sistema de reporter√≠a con 26 parsers totales (5 existentes + 21 nuevos).

---

**Versi√≥n**: 1.0  
**Autor**: Claude AI  
**Fecha**: 10 Diciembre 2025